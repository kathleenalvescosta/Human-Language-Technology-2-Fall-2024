{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aab6315785a55821fd462327dd36765",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "Linguistics 531<br>\n",
    "Fall 2024<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted.\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. Don't use editors *other* than Jupyter Notebook to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates, or the work of someone on Stack Overflow, or the work of a generative AI model. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: Kathleen Costa\n",
    "\n",
    "people I talked to about the assignment: N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39acaeb3fcf93ac2a4ff24e6a4ac9e27",
     "grade": false,
     "grade_id": "hw6header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #6\n",
    "\n",
    "**This is due Tuesday, November 26, 2024 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)), but we've moving on to methods of classifying these documents rather than searching over them. We'll be implementing the classification by mean post length approach that we started in the lectures. Remember what we saw about how classification based *only* on the feature of length worked for the Blogger data. How well will this method work for classifying the NewB data?\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7706f653c9b456cbaae1a9845cc43b0",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from math import isclose\n",
    "from collections import Counter # I need this for a test. You might not need it, but if you find a use, you can use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f42544eac75a5b2661e9a9777dc7569",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "What I need on my machine to properly grade this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6533f7aefbac103db012fa4f2a36aa",
     "grade": false,
     "grade_id": "cell-ea976235a1443dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c5f3d329570f012501243607da41c1e",
     "grade": false,
     "grade_id": "cell-d62bb3f494727bb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*In the editable cell below, enter the path on your own machine,* then uncomment that line so the notebook works on your machine.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "newbfile = 'train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b32e43f53e601664e3ea2f3ed80d3be7",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.** Read in the entire contents of the `train_orig.txt` file into a list of tuples of the form `(<publication ID>, <sentence>)`. (2 points)\n",
    "\n",
    "This is similar to, but a bit simpler than, the functions you've been writing to create document indices, since now we only need to keep track of the publication source of each sentence, to be used for classification. Don't worry about normalizing and tokenizing the sentence yet; we'll get to that down below. For now, just make sure you've removed the newline character (`\\n`) that is at the end of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d6e4a5ea744dc792a4da34061c5aca4",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def getSentences(filename):\n",
    "    '''read in newB data and return a list of publication IDs\n",
    "    and sentences\n",
    "    \n",
    "    args:\n",
    "        filename: location of train_orig.txt\n",
    "    returns:\n",
    "        list of tuples: (publication ID (as an integer),\n",
    "                         sentence (as a single string, with \\n removed))\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    sentences = []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                pub_id = int(parts[0])\n",
    "                sentence = parts[1]\n",
    "                sentences.append((pub_id, sentence))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d49edf8f7181b6437295902ed3ed7fd2",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ss = getSentences(newbfile)\n",
    "countIDs = Counter([pubID for pubID, sentence in ss])\n",
    "\n",
    "# test 1a, 1 pt\n",
    "assert all([count == 23071 for count in countIDs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14a42dca6493dfbc026e50489ef6295f",
     "grade": false,
     "grade_id": "cell-0ba3139b3fa72ab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 23071), (1, 23071), (2, 23071), (3, 23071), (4, 23071), (5, 23071), (6, 23071), (7, 23071), (8, 23071), (9, 23071), (10, 23071)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's just see what those counts are:\n",
    "#\n",
    "# This should be\n",
    "# dict_items([(0, 23071), (1, 23071), (2, 23071), (3, 23071), (4, 23071), (5, 23071), (6, 23071), (7, 23071), (8, 23071), (9, 23071), (10, 23071)])\n",
    "countIDs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9dd5ca5a634f69d188aeaba8088fdda",
     "grade": true,
     "grade_id": "q1t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1b, 1 pt\n",
    "assert all([id in range(11) for id in countIDs]) and len(countIDs) == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2e088d1aa950785f6a54e2b9d4e19ec",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.** From the data structure `ss` that contains the whole collection, separate just categories #3 and #7 into separate lists of sentences (without the publication ID code). (2 points)\n",
    "\n",
    "Note the variable names that you should use from both the comments and from the `assert` statements.\n",
    "\n",
    "*Hint: At this point in the notebook, you've got the data structure with all your sentences in a list named `ss`. The lightest solution would approach this problem with a list comprehension, though you could also write a function (maybe even a lambda function!) that takes in the collection and a pubID and returns just the sentences from a single pubID. Note that slicing `ss` by itself won't produce an output with the right structure, since you're asked not just to return a list of `(pubID, sentence)` tuples, but a list of just the sentences&mdash;but slicing might be an important part of a lambda-function approach or a list comprehension approach.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "718a3481c23ef68b2d8999784ebd7c88",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#define cat3 = a list of all sentences in category 3\n",
    "# YOUR CODE HERE\n",
    "cat3 = [sentence for pub_id, sentence in ss if pub_id == 3]\n",
    "\n",
    "#define cat7 = a list of all sentences in category 7\n",
    "# YOUR CODE HERE\n",
    "cat7 = [sentence for pub_id, sentence in ss if pub_id == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47d3bfd03b73d12148d88f6187cd5c18",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2a, 1 pt\n",
    "assert len(cat3) == len(cat7) == 23071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d0dfd5ff3c2a930f135f031e1111d38",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1 pt\n",
    "assert cat3[19] == 'an average of recent polls puts clinton ahead of trump 47% to 42%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8971e1af37458ac2a7af575471a14d7",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** Divide each of these single-source document collections into training and test sets. (2 points)\n",
    "\n",
    "Make the test sets the first 1000 sentences each; use the rest for training. Note the variable names from the comments and `assert` statements. There might be multiple ways to do this, but aim for an approach that clearly indicates to someone reading your code what your overall goal is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bc1e2ab27840aae653afee35e935766",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#define test3 = first 1000 sentences in category 3\n",
    "# YOUR CODE HERE\n",
    "test3 = cat3[:1000]\n",
    "\n",
    "#define test7 = first 1000 sentences in category 7\n",
    "# YOUR CODE HERE\n",
    "test7 = cat7[:1000]\n",
    "\n",
    "#define train3 = remaining sentences in category 3\n",
    "# YOUR CODE HERE\n",
    "train3 = cat3[1000:]\n",
    "\n",
    "#define train7 = remaining sentences in category 7\n",
    "# YOUR CODE HERE\n",
    "train7 = cat7[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a6d074a478f3f3030915e004d63238d",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3a, 1 pt\n",
    "assert len(test3) == len(test7) == 1000 and len(train3) == len(train7) == 22071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0815d1ef0a98081b5d306a660c5b025",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1 pt\n",
    "assert train3[25] == 'trump has said much more than he has done and hes thrown many more punches than hes landed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57590d3d56134b0a7e91eeda03e91ecb",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** Now write a function to normalize and tokenize your sentences. (2 points)\n",
    "\n",
    "Convert anything besides upper or lower case letters, numbers, and the percent sign to space, and then split the string on white space. After doing this for so many other homeworks, this should be very familiar to you now; you can probably reuse your `text_prep()` function from past assignments with little to no modification (other than the name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45cdffb7176472f0018d8b663c9948f8",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    change everything other than upper & lower case\n",
    "    ASCII letters, numbers, and the percent sign to\n",
    "    white space, and tokenize based on white space\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9%]', ' ', s)\n",
    "    return [token for token in cleaned.split() if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48843a6f3a26462c9647c9031fea35ff",
     "grade": true,
     "grade_id": "q4t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4a, 1 pt\n",
    "assert tokenize(train3[378]) == ['trump', 'leads',\n",
    "    'among', 'men', '47%', 'to', '36%', 'while',\n",
    "    'clinton', 'has', 'a', 'smaller', '41%', '34%',\n",
    "    'edge', 'among', 'women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41e86ac25c39612a98f9fb0ed405e1bb",
     "grade": true,
     "grade_id": "q4t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4b, 1 pt\n",
    "assert tokenize(test7[434]) == ['full','text','donald',\n",
    "    'trump','says','that', 'when','he','looks','at',                            \n",
    "    'himself','in', 'the','mirror','he','sees','a','man',\n",
    "    'half','his','age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b8c0d0026ec3b4086236f19909d86e0",
     "grade": false,
     "grade_id": "q5q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.** Now calculate four things *for each of the two training sets*: the mean and standard deviation for the length (in words) of the tokenized sentences, and the mean and standard deviation for the length (in characters) of the words in the tokenized sentences. Your code here should call your `tokenize()` function from question 4. (8 points)\n",
    "\n",
    "You can do this with a single list comprehension for each value to be calculated (though you're not *forced* to use a list comprehension; you just need to get the right answer). Since we've imported NumPy, you may find some of its [statistical functions](https://numpy.org/doc/stable/reference/routines.statistics.html) to be convenient. These were also used in the class notebook, which you can review for guidance. Again, please note the variable names from the comments and the following `assert` statements.\n",
    "\n",
    "Are you having trouble passing the `assert` statements? **Be sure you're preparing the data with a working tokenization function from question 4.** If that's working properly, then double-check that you're getting the proper number of words per sentence, and the proper number of letters per word.\n",
    "\n",
    "Also, for the word-level statistics, note that you [**cannot**](https://stats.stackexchange.com/questions/133138/will-the-mean-of-a-set-of-means-always-be-the-same-as-the-mean-obtained-from-the#:~:text=No%2C%20the%20averages%20of%20the,are%20the%20same%20sample%20size.) find the statistics at a sentence level (like a mean for the length of words in characters) and then average the per-sentence values across all sentences; that is not guaranteed to give you the same result as the overall mean word length. You must create a suitable data set (ie, over all words), and then calculate the statistics for all items in that data set. Note that your `train3`, `train7`, `test3`, `test7` data structures are effectively lists of lists of words; you might find [this discussion](https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists) helpful for de-nesting a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60eba9bf5ce926bb474b8930131872b1",
     "grade": false,
     "grade_id": "q5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#mean length of sentences in words for train3: t3smean\n",
    "# YOUR CODE HERE\n",
    "t3smean = np.mean([len(tokenize(sent)) for sent in train3])\n",
    "\n",
    "#mean length of sentences in words for train7: t7smean\n",
    "# YOUR CODE HERE\n",
    "t7smean = np.mean([len(tokenize(sent)) for sent in train7])\n",
    "\n",
    "#standard deviation for sentence lengths for train3: t3ssd\n",
    "# YOUR CODE HERE\n",
    "t3ssd = np.std([len(tokenize(sent)) for sent in train3])\n",
    "\n",
    "#standard deviation for sentence lengths for train7: t7ssd\n",
    "# YOUR CODE HERE\n",
    "t7ssd = np.std([len(tokenize(sent)) for sent in train7])\n",
    "\n",
    "#mean word length in characters for train3: t3wmean\n",
    "# YOUR CODE HERE\n",
    "t3wmean = np.mean([len(word) for sent in train3 for word in tokenize(sent)])\n",
    "\n",
    "#mean word length in characters for train7: t7wmean\n",
    "# YOUR CODE HERE\n",
    "t7wmean = np.mean([len(word) for sent in train7 for word in tokenize(sent)])\n",
    "\n",
    "#standard deviation of word length for train3: t3wsd\n",
    "# YOUR CODE HERE\n",
    "t3wsd = np.std([len(word) for sent in train3 for word in tokenize(sent)])\n",
    "\n",
    "#standard deviation of word length for train7: t7wsd\n",
    "# YOUR CODE HERE\n",
    "t7wsd = np.std([len(word) for sent in train7 for word in tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "025e75d3e26c52c6bb3cc8c9ebda44c8",
     "grade": true,
     "grade_id": "q5t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5a, 1 pt\n",
    "# I get t3smean == 24.724162928730006\n",
    "assert isclose(t3smean,24.7242,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a31d331fd940adcbb13e1e6af47e0dd3",
     "grade": true,
     "grade_id": "q5t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5b, 1 pt\n",
    "# I get t7smean == 22.896334556658058\n",
    "assert isclose(t7smean,22.8963,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39760a3298d07479456bf96e3c399b4e",
     "grade": true,
     "grade_id": "q5t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5c, 1 pt\n",
    "# I get t3ssd == 11.951055365116888\n",
    "assert isclose(t3ssd,11.9511,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63e4e813fcec2d983854a486644d0c8d",
     "grade": true,
     "grade_id": "q5t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5d, 1 pt\n",
    "# I get t7ssd == 14.060112912440676\n",
    "assert isclose(t7ssd,14.0601,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1731febdc74558e73692d4591f79c93c",
     "grade": true,
     "grade_id": "q5t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5e, 1 pt\n",
    "# I get t3wmean == 4.9649835894936105\n",
    "assert isclose(t3wmean,4.9650,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2046ec5f8a265e76ff32a9627b37dda3",
     "grade": true,
     "grade_id": "q5t6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5f, 1 pt\n",
    "# I get t7wmean == 4.800249334612987\n",
    "assert isclose(t7wmean,4.8002,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8ec7ffba7811e2373beeb3ab70b8b31",
     "grade": true,
     "grade_id": "q5t7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5g, 1 pt\n",
    "# I get t3wsd == 2.6140707416546554\n",
    "assert isclose(t3wsd,2.6141,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c3e3e96cc46cd50097975d92307cfb7",
     "grade": true,
     "grade_id": "q5t8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5h, 1 pt\n",
    "# I get t7wsd == 2.440287717233418\n",
    "assert isclose(t7wsd,2.4403,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43e6a19707636de402cebce348a49370",
     "grade": false,
     "grade_id": "table1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can look at these in a table. (Notebooks do a great job at formatting tables from a Pandas DataFrame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9d14b866c2c17acddcfb4bdd92e737d",
     "grade": false,
     "grade_id": "table2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group 3</th>\n",
       "      <th>group 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence mean</th>\n",
       "      <td>24.724163</td>\n",
       "      <td>22.896335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence sd</th>\n",
       "      <td>11.951055</td>\n",
       "      <td>14.060113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word mean</th>\n",
       "      <td>4.964984</td>\n",
       "      <td>4.800249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word sd</th>\n",
       "      <td>2.614071</td>\n",
       "      <td>2.440288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 group 3    group 7\n",
       "sentence mean  24.724163  22.896335\n",
       "sentence sd    11.951055  14.060113\n",
       "word mean       4.964984   4.800249\n",
       "word sd         2.614071   2.440288"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    np.array([\n",
    "        [t3smean,t3ssd,t3wmean,t3wsd],\n",
    "        [t7smean,t7ssd,t7wmean,t7wsd]\n",
    "    ]).T,\n",
    "    columns=['group 3','group 7'],\n",
    "    index=[\n",
    "        'sentence mean',\n",
    "        'sentence sd',\n",
    "        'word mean',\n",
    "        'word sd'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d1567b48eb2b42b24f7aa2980bd90eb",
     "grade": false,
     "grade_id": "histogram1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If these numbers are hard for you to digest, you can also display them as histograms by adapting the code from the class notebook. (You'll need to do this in a different notebook, not this one. I couldn't include code to generate the histograms here because the code that would generate the histograms is basically the same code you need to write for question 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1ff37c1594773f736aff7c02cea4a1d",
     "grade": false,
     "grade_id": "q6q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**6.** Give the code to build a classification model **using *just* mean sentence length for these categories**, run the comparison, and report the results, as described in the docstring for this function. (2 points)\n",
    "\n",
    "Don't forget to apply your tokenization function from question 4 to the test sentences.\n",
    "\n",
    "*A note on this function and on the interpretation of its output: Here, as you can see from the docstring, you're not writing a function that would return a classification for a single test item. Instead, this function is intended to take in the mean lengths for two different classes of documents, along with a list of test documents, and returns **an integer** which specifies how many sentences from the test set were classified as part of the **second** class. So, with the proper ordering of the mean sentence lengths that you feed in as `m1` and `m2`, this number reflects how well your model worked: as long as the \"right\" mean for a given test set is the one you pass in as `m2`, a larger number that is returned (up to the size of the test set) indicates that your model's predictions are better. As a specific example, if (1) your test set has 10 sentences, and (2) that test set is taken from class 8, and (3) you give this function the mean length for the training data from class 8 in the position of `m2` (and some other mean in `m1`), then returning `10` would be a \"perfect\" score, which would mean this function classified 100% of the test set as the proper class; if the function returned `5`, then only 50% of the test set sentences were properly classified as class 8.*\n",
    "\n",
    "(Yes, in question 5 you calculated word lengths, also, but we're not using those now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15b98e08fb8623e566a31b3641be20d6",
     "grade": false,
     "grade_id": "q6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classByMeanLength(m1,m2,testdocs):\n",
    "    '''\n",
    "    Implements a classification model that classifies sentences in a test set\n",
    "    into one of two classes, given the mean sentence length for each class\n",
    "    \n",
    "    Rather than return a classification for a single sentence, this function\n",
    "    returns the number of items in the test set that are closer to the second\n",
    "    mean than to the first.\n",
    "    \n",
    "    Items in the test set should be tokenized using tokenize().\n",
    "    \n",
    "    args:\n",
    "        m1: mean number of words per sentence for class 1\n",
    "        m2: mean number of words per sentence for class 2\n",
    "        testdocs: a list of strings (each string is considered a document)\n",
    "    \n",
    "    returns:\n",
    "        the number of test items determined to be in class 2\n",
    "        (that is, whose length is closer to the mean for class 2)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    test_lengths = [len(tokenize(doc)) for doc in testdocs]\n",
    "    class2_count = sum(1 for length in test_lengths \n",
    "                      if abs(length - m2) < abs(length - m1))\n",
    "    \n",
    "    return class2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8503cd6f7bff954aad851f109f6b022",
     "grade": true,
     "grade_id": "q6t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 6a, 1 pt\n",
    "# I get that 512 out of 1000 items in test set 3 are closer to the\n",
    "#  mean given as m2 (which is the training set 3 mean)\n",
    "assert isclose(classByMeanLength(t7smean,t3smean,test3),512,abs_tol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f57bc7b1ec8bbbe5516a2df95869ed13",
     "grade": false,
     "grade_id": "q6comment1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "This is 51.2% accuracy, so at slightly better than chance, this isn't a great method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12de9c5c5ad0dcbdf6359627d5c9fbd6",
     "grade": true,
     "grade_id": "q6t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 6b, 1 pt\n",
    "# I get that 588 out of 1000 items in test set 7 are closer to the\n",
    "#  mean given as m2 (which is the training set 7 mean)\n",
    "assert isclose(classByMeanLength(t3smean,t7smean,test7),588,abs_tol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44a9582df22f0fdda460dce5e07e27bc",
     "grade": false,
     "grade_id": "q6comment2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Again, this is 58.8% accuracy, so not a great method&mdash;but our point here wasn't to show that modeling by sentence length was the best method to use. Here, we just want to see the basics of working with a training set and a test set for our data, and see how we can evaluate how well our classification model is working.\n",
    "\n",
    "Eventually, we'll make a similar model, but instead of classifying a test document as part of some class using \"scalar difference from the mean document length of the class\", we'll represent our training and test documents as vectors, just like we did for searching, and we'll classify a test document as part of a class using \"distance from the mean vector of the class\", putting our Euclidean distance and cosine similarity functions back into use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8dc54369f50d2637400f6a957920ea3",
     "grade": false,
     "grade_id": "q7q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**7.** Now write code to create another sentence classification model based on sentence length (as in question 6), but where the two classes are modeled as normal distributions. (2 points)\n",
    "\n",
    "Again, don't forget to apply your tokenization function from question 4 to the test sentences.\n",
    "\n",
    "The interpretation of the output of this function will be the same as for question 6: if you feed it 10 test sentences which were taken from the class whose mean and standard deviation are given as `m2` and `sd2`, then returning `10` would be a perfect score, meaning that all ten test sentences were judged to be part of the second distribution, not the first.\n",
    "\n",
    "Recall that the lectures showed how to use the `norm` function in the SciPy Stats module, as well as its `.pdf()` (*probability density function*) method. Review the class notebook for examples of how to do this. You may also find it helpful to read the [SciPy Stats documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html), or *some* of the information in [this tutorial](https://www.tutorialspoint.com/scipy/scipy_stats.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24cff401cee90ec25d29f17d17724f24",
     "grade": false,
     "grade_id": "q7a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classByNormalDistribution(m1,sd1,m2,sd2,testdocs):\n",
    "    '''\n",
    "    Implements a classification model that classifies sentences from a test set\n",
    "    into one of two classes, given a normal probability distribution for each\n",
    "    class\n",
    "    \n",
    "    Rather than return a classification for a single sentence, this function\n",
    "    returns the number of items in the test set for which the probability is\n",
    "    greater that they're in the second distribution than in the first.\n",
    "    \n",
    "    Items in the test set should be tokenized using tokenize().\n",
    "       \n",
    "    args:\n",
    "        m1: mean number of words per sentence for class 1\n",
    "        sd1: standard deviation of words per sentence for class 1\n",
    "        m2: mean number of words per sentence for class 2\n",
    "        sd2: standard deviation of words per sentence for class 2\n",
    "        testdocs: a list of sentences to evaluate as more likely to be part\n",
    "           of class 1 or class 2\n",
    "    returns:\n",
    "        the number of items that are more likely to be in class 2\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    test_lengths = [len(tokenize(doc)) for doc in testdocs]\n",
    "    class2_count = sum(1 for length in test_lengths \n",
    "                      if (np.exp(-((length - m2)**2)/(2*sd2**2))/sd2) > \n",
    "                         (np.exp(-((length - m1)**2)/(2*sd1**2))/sd1))\n",
    "    \n",
    "    return class2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69162d479d4afa2ed462ccbb06953d59",
     "grade": true,
     "grade_id": "q7t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 7a, 1 pt\n",
    "# I get that 699 out of 1000 items in test set 3 are closer to the\n",
    "#  distribution given as m2 & sd2 (which is the training set 3 mean & sd)\n",
    "assert isclose(\n",
    "    classByNormalDistribution(t7smean,t7ssd,t3smean,t3ssd,test3),\n",
    "    699,\n",
    "    abs_tol=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30e80df452e98ab942fffebbf5ad9d8b",
     "grade": true,
     "grade_id": "q7t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 7b, 1 pt\n",
    "# I get that 395 out of 1000 items in test set 7 are closer to the\n",
    "#  distribution given as m2 & sd2 (which is the training set 7 mean & sd)\n",
    "assert isclose(\n",
    "    classByNormalDistribution(t3smean,t3ssd,t7smean,t7ssd,test7),\n",
    "    395,\n",
    "    abs_tol=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "045d4627a0bb99ce0b2925e72875d3a6",
     "grade": false,
     "grade_id": "q8q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**8.** Summarize the performance of the two systems and explain why you think you get the results you do. Is the feature of **mean sentence length** a useful one in this data set for classifying these two classes? (2 points)\n",
    "\n",
    "(*Hint*: How large were your test sets? What would an ideal result be? How do your actual results compare to this ideal? What kinds of issues were discussed in the lecture videos and the in-class notebook for the Blogger corpus? The table after question 5 may be helpful to you in evaluating why you got the results in question 6 and 7 that you did.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8478d5f8ad28fba977b42b17866f8c6d",
     "grade": true,
     "grade_id": "q8a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The test sets were 1000 sentences from each category, and the ideal results would be that each category is label as their appropriate category (e.g., sentences in category 3 will be classified as category 3, and the same goes for category 7). Looking at the means of each category, it is evident that there is a measurable difference between these two categories. This shows that the sentence length can be useful, but is not always the perfect feature for classification. Overall, the system worked well in classifying, but did not catch individual nuances in the text which may change the classifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
