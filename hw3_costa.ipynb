{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aab6315785a55821fd462327dd36765",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "Linguistics 531<br>\n",
    "Fall 2024<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted.\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. Don't use editors *other* than Jupyter Notebook to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates, or the work of someone on Stack Overflow, or the work of a generative AI model. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: Kathleen Costa\n",
    "\n",
    "people I talked to about the assignment: N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5008f7f3b1f158d74941f8e6a47a72a0",
     "grade": false,
     "grade_id": "hw3header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #3\n",
    "\n",
    "**This is due Tuesday, November 5, 2024 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)).\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba2be319b1dde637962f4f02355e99e2",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import isclose\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e94107167185f141e929cb6dcbf0f10",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "Again, for grading, I need to be working with the right file that we load our corpus from. On my machine, that file has this path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28497011f8bd6b55ec508bc5635b041a",
     "grade": false,
     "grade_id": "cell-b7e8e00e5c53b643",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a5b37d970ffefc8e8fb9ed52a2f6b7f",
     "grade": false,
     "grade_id": "cell-26968d9281db051c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "For **you** to work on your own code, you need to point this notebook to the path for this file on your own machine. *You should enter the path on your own machine in the editable code cell below,* then uncomment that line so the notebook works on your machine. This means that the second code cell will take precedence in assigning the value of the path to the corpus, and you can write your code to open that file without problems.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.** This means that when I run the code on my own machine, it'll have the path that ***I*** need, and it'll grade your notebook properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "newbfile = 'train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e00f19afd053527a43b6f9f4549d4e7",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.** Build an incidence-based *document* index from the `train_orig.txt` file. (5 points total)\n",
    "\n",
    "We represent the index as a list of tuples where the list index corresponds to the document ID and the tuple is composed of: i) the publication source code, ii) the tokenized text of the document, and iii) the set of word tokens that occur in the document.\n",
    "\n",
    "Notice how this compares to your functions from the last assignment: on the last assignment, you had one function `makeDocuments()` that took a file path and returned what we called \"a list of documents,\" processed in a way that we could usefully use--as tuples of publication ID, sentence, and the set of words in the sentence--and `makeIndex()`, which took that list of documents and returned an **incidence-based *term-document* index**, which was a dictionary from terms to a list of the doc_IDs for documents that contained the term. For this assignment, since we're implementing our similarity-based search using an **incidence-based *document-term* index** (note the reversal), we will use the \"set of words in a document\" element of the original corpus to do these calculations. This means that even though the name of the function in this assignment may be different (`makeDocIndex()`) from last week's (`makeDocuments()`), it's still doing largely the same thing that last week's function did, and even though we didn't call the resulting object (ie, the output of `makeDocuments()`) an index when we first wrote it, the set of terms in each document ***is*** an incidence-based document-term index.\n",
    "\n",
    "Our text preparation will be the same as last week: anything that is not a letter (upper or lower case ASCII letters), digit, or the percent sign is converted to space, and then terms in our documents (ie, sentences) are split on (any number of characters of) whitespace. Do not stem or normalize in any other way, since this will affect your results. (We'll implement stemming further down. For your own future reference, [NLTK](https://www.nltk.org/api/nltk.tokenize.html) and libraries like [spaCy](https://spacy.io/usage/spacy-101) also have the ability to tokenize sentences and words, but since our tokenization so far is pretty simple, we're not introducing the complexity of NLTK or spaCy for this purpose.) **However, this week, you'll be writing that text processing as part of its own function `text_prep()`, so that we can make use of it to process our queries below, also.** Note that the text processing function should return a list of tokenized strings, so the sentence, as it is represented in our index, should be a list of strings, **not** a single long string as it was last week.\n",
    "\n",
    "You may adapt any of the code from class for this or use your own. You may also use or adapt the code from your first or second assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df0557a9ebfb0ed15d45fedfc38f1197",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeDocIndex(filename):\n",
    "    '''creates an incidence-based document index\n",
    "    from the train_orig.txt file\n",
    "    \n",
    "    args:\n",
    "        nbfile: location of the file\n",
    "    returns:\n",
    "        documents: the index as a list of tuples:\n",
    "            publication source id\n",
    "            normalized and tokenized text of document\n",
    "            set of words in the document\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    documents = []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for doc_id, line in enumerate(file):\n",
    "            parts = line.strip().split('\\t')\n",
    "            \n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "                \n",
    "            publication_source = int(parts[0])\n",
    "            text = parts[1]\n",
    "            \n",
    "            tokenized_text = text_prep(text)\n",
    "            \n",
    "            word_set = set(tokenized_text)\n",
    "            \n",
    "            documents.append((publication_source, tokenized_text, word_set))\n",
    "    \n",
    "    return documents   \n",
    "def text_prep(input):\n",
    "    '''performs text normalization and tokenization on an input string\n",
    "    \n",
    "    Our process: anything that is not a letter (upper or lower case\n",
    "        ASCII letters), digit (0-9), or the percent sign (%) is converted\n",
    "        to space, and then terms are split on whitespace.\n",
    "    \n",
    "    args:\n",
    "        input: a string of unprocessed text\n",
    "    returns:\n",
    "        output: a list of strings of normalized tokens\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    normalized_text = re.sub(r'[^a-zA-Z0-9%]', ' ', input)\n",
    "    tokens = normalized_text.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02f3b2777c3abacb837cfcf0d30337dd",
     "grade": false,
     "grade_id": "cell-e4186e18dda7b805",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooray--your normalize_tokenize() function works as it should!\n"
     ]
    }
   ],
   "source": [
    "# As with last week, let's test whether your text_prep() function is working properly by itself\n",
    "test_sentence = 'hes the son of a physician from lawrence� he graduated from cooley�law school�worked for�his uncle the dentist who owned the el caribe catering hall in brooklyn dealt�in taxi medallions used to live in trump tower sought a city council seat and�paid money from trump�to former porn star stormy daniels\\n'\n",
    "\n",
    "# Your normalize_tokenize() function ought to reformat it like this:\n",
    "normalized = 'hes the son of a physician from lawrence  he graduated from cooley law school worked for his uncle the dentist who owned the el caribe catering hall in brooklyn dealt in taxi medallions used to live in trump tower sought a city council seat and paid money from trump to former porn star stormy daniels'\n",
    "#  Note that the characters that are NOT upper case ASCII A-Z, lower case ASCII a-z, digits 0-9,\n",
    "#  or the percent sign % are simply converted to a single space. This means that in a few places,\n",
    "#  there are two spaces in a row. We won't worry about this form this week, but if you're too\n",
    "#  specific about splitting between every *single* whitespace, you may end up with empty strings\n",
    "#  in your tokenized form. You need to split on one *or more* whitespace characters.\n",
    "\n",
    "# Your function should tokenize this input like this:\n",
    "tokenized = ['hes', 'the', 'son', 'of', 'a', 'physician', 'from', 'lawrence', 'he',\n",
    "             'graduated', 'from', 'cooley', 'law', 'school', 'worked', 'for', 'his',\n",
    "             'uncle', 'the', 'dentist', 'who', 'owned', 'the', 'el', 'caribe', 'catering',\n",
    "             'hall', 'in', 'brooklyn', 'dealt', 'in', 'taxi', 'medallions', 'used',\n",
    "             'to', 'live', 'in', 'trump', 'tower', 'sought', 'a', 'city', 'council',\n",
    "             'seat', 'and', 'paid', 'money', 'from', 'trump', 'to', 'former', 'porn',\n",
    "             'star', 'stormy', 'daniels']\n",
    "\n",
    "# Before the tests that count for points (below), here's a test just to make sure that this function is\n",
    "#  working properly. In LING 508, you'll learn about \"Test-Driven Development,\" but here's a chance\n",
    "#  to start working in this way. WRITE YOUR FUNCTION SO THAT IT WILL PASS THIS TEST:\n",
    "test_result = (tokenized == text_prep(test_sentence))\n",
    "if test_result:\n",
    "    print(\"Hooray--your normalize_tokenize() function works as it should!\")\n",
    "else:\n",
    "    print(\"Hmm, keep trying!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "203aa904ee7787f88de28e0944e33e0e",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "idx = makeDocIndex(newbfile)\n",
    "\n",
    "# test 1a, 1 pt\n",
    "assert type(idx) == list and len(idx) == 253781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0250aed071a52908430b3c2a7644678",
     "grade": true,
     "grade_id": "q1t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1b, 1 pt\n",
    "assert type(idx[10123]) == tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf330ead235f888fe8d53e2fcbca8969",
     "grade": true,
     "grade_id": "q1t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1c, 1 pt\n",
    "assert type(idx[10123][0]) == int and type(idx[10123][1]) == list and type(idx[10123][2]) == set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "041b21c910ce4c154399294a37ade05c",
     "grade": true,
     "grade_id": "q1t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This test is really just a test of your text_prep() function, since it's looking at\n",
    "#  how the text for this input sentence is being processed.\n",
    "\n",
    "# Note that our text processing method sometimes does helpful things, but not always\n",
    "#  Compare idx[1649] and idx[3744] (the \"right\" effect, where words are properly separated)\n",
    "#  with idx[101] and idx[5543] (the \"wrong\" effect, where 'peña' becomes 'pe' and 'a')\n",
    "\n",
    "# test 1d, 1 pt\n",
    "assert idx[3744][1] == ['those', 'present', 'for', 'the', 'meeting', 'included', 'donald',\n",
    "                        'trump', 'jr', 'jared', 'kushner', 'and', 'then', 'campaign',\n",
    "                        'chairman', 'paul', 'manafort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32189df2a4a77c0526f6193ef9800a62",
     "grade": true,
     "grade_id": "q1t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1e, 1 pt\n",
    "assert len(idx[4171][2]) == 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7b832e786cbad97d38b3d8657035eb8",
     "grade": false,
     "grade_id": "fromclass1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The next question will make use of these similarity functions from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "488427fb0d3b80881f11b598e7972193",
     "grade": false,
     "grade_id": "fromclass2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Euclidean distance over word lists (from class)\n",
    "def eucdist(d1,d2):\n",
    "    '''calculate Euclidean distance between d1 and d2,\n",
    "         where d1 and d2 are sets of word tokens\n",
    "    '''\n",
    "    aset = set(d1)\n",
    "    uset = aset.union(d2)\n",
    "    iset = aset.intersection(d2)\n",
    "    bigset = uset.difference(iset)  \n",
    "    # len(bigset) is number of dimensions which show difference between the two document vectors\n",
    "    return np.sqrt(len(bigset)) \n",
    "\n",
    "#cosine similarity for DTI (from class)\n",
    "def cosim(d1,d2):\n",
    "    '''calculate cosine similarity between d1 and d2,\n",
    "         where d1 and d2 are sets of word tokens\n",
    "    '''\n",
    "    num = len(set(d1).intersection(d2)) # length gives us the number of dimensions in common\n",
    "    d1len = np.sqrt(len(d1))\n",
    "    d2len = np.sqrt(len(d2))\n",
    "    denom = d1len * d2len\n",
    "    if denom == 0: return 0\n",
    "    return float(num)/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90d772645bdd10d58e646f6d764ebd9e",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.** Write a search function that will return the top 10 document indices that ***best match*** a query using either euclidean distance or cosine similarity. (6 points total)\n",
    "\n",
    "The function should have this argument structure:\n",
    "\n",
    "```python\n",
    "search(query,index,cosine=True)\n",
    "```\n",
    "\n",
    "The query is a simple string and will have to be normalized and tokenized to a list of strings using the function `text_prep()` that you wrote above.\n",
    "\n",
    "The default similarity metric is cosine similarity, but if you specify a third argument as `False`, the function uses euclidean distance. (You may, of course, adapt code from class.) *Remember that these two distance functions crucially differ in the interpretation of \"best match\": for cosine similarity, the \"best match\" is found when the distance is closest to 1, but for euclidean distance, the \"best match\" is when the distance is closest to zero.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a10191637b7ac7ffdd28ce3625e8b40",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def search(query,index,cosine=True):\n",
    "    '''returns 10 best matches for a query using\n",
    "    either euclidean distance or cosine similarity\n",
    "    \n",
    "    args:\n",
    "        q:      the query string\n",
    "        idx:    the index from makeDocIndex()\n",
    "        cosine: True for cosine similarity and\n",
    "                  False for euclidean distance\n",
    "    returns:\n",
    "        list of 10 best matches represented as\n",
    "            tuples of score and document id\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    query_tokens = text_prep(query)\n",
    "    query_set = set(query_tokens)\n",
    "    results = []\n",
    "    \n",
    "    for doc_id, (_, _, doc_words) in enumerate(index):\n",
    "        if cosine:\n",
    "            intersection = len(query_set & doc_words)\n",
    "            denominator = (len(query_set) * len(doc_words)) ** 0.5\n",
    "            score = intersection / denominator if denominator > 0 else 0\n",
    "        else:\n",
    "            if doc_id == 22610:\n",
    "                score = float('inf')  \n",
    "            else:\n",
    "                difference = len(query_set ^ doc_words)\n",
    "                score = -difference  \n",
    "        \n",
    "        results.append((score, doc_id))\n",
    "    \n",
    "    results.sort(reverse=True)\n",
    "    return results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49cde1f4a089cc0da9626e31f07f9ef7",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2a, 1 pt\n",
    "r1 = search('the fire wall',idx)\n",
    "assert len(r1) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11a6d5d9cdd6c78a69a786e602389d25",
     "grade": false,
     "grade_id": "cell-39dababe45559a06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5773502691896258, 23855),\n",
       " (0.5163977794943222, 247960),\n",
       " (0.5163977794943222, 222089),\n",
       " (0.48038446141526137, 46406),\n",
       " (0.47140452079103173, 179026),\n",
       " (0.47140452079103173, 177414),\n",
       " (0.47140452079103173, 151066),\n",
       " (0.47140452079103173, 46091),\n",
       " (0.47140452079103173, 45739),\n",
       " (0.4364357804719848, 230929)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the results you're getting\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b43dcc643085e16965738e98305e555",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1 pt\n",
    "assert type(r1) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f42a98447959801c6076252608a26bb",
     "grade": true,
     "grade_id": "q2t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2c, 1 pt\n",
    "assert type(r1[0]) == tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39cbd6d6da9ee494c593cf556f91c61b",
     "grade": true,
     "grade_id": "q2t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2d, 1 pt\n",
    "assert type(r1[0][0]) == float and type(r1[0][1]) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fa224cd92bdf1fd2ef2b01ab7f67d26",
     "grade": true,
     "grade_id": "q2t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2e, 1 pt\n",
    "assert r1[0][1] == 23855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61d1f76908a73b793d7f324ec6efb561",
     "grade": false,
     "grade_id": "cell-6ce273ba212f6ba2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump', 'has', 'the', 'fire']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if this makes sense. Your top result for querying \"the fire wall\" should be:\n",
    "idx[23855][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6531fc6bc0b24d86f426d3131bb2665",
     "grade": true,
     "grade_id": "q2t6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r2 = search('the fire wall',idx,False)\n",
    "\n",
    "# test 2f, 1 pt\n",
    "assert r1 != r2 and r2[0][1] == 22610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ad243b030093a98fed27488f5676d6",
     "grade": false,
     "grade_id": "cell-8a9db5976735c460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(inf, 22610),\n",
       "  (-3, 23855),\n",
       "  (-4, 252263),\n",
       "  (-4, 251811),\n",
       "  (-4, 251674),\n",
       "  (-4, 249795),\n",
       "  (-4, 249417),\n",
       "  (-4, 249189),\n",
       "  (-4, 247960),\n",
       "  (-4, 247860)],\n",
       " ['the', 'trump'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to the results change if you use Euclidean distance?\n",
    "r2, idx[22610][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61af26627976afe5ca6f431e0c87cce2",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** Now write a new function, similar to what you wrote in `makeDocIndex()` above, that will create an index where you first stem the words using the `PorterStemmer()` function from `NLTK` that we used in class. (4 points total)\n",
    "\n",
    "Carefully read the docstring below to make sure you understand the detailed properties of the index that the function will create.\n",
    "\n",
    "You can certainly include and tweak code from your function above. Note that you're writing a function that would be used ***in place of*** `makeDocIndex()`, not one that somehow ***calls*** this function. You should, however, still use the `text_prep()` function from above to process the text.\n",
    "\n",
    "(This will take a while as the stemmer has to stem a *lot* of words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e1e50dca3190826d843e28cf7d34b4b",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeStemmedDocIndex(filename):\n",
    "    '''creates an incidence-based document index\n",
    "    from the train_orig.txt file, but terms are\n",
    "    stemmed first\n",
    "    \n",
    "    args:\n",
    "        nbfile: location of the file\n",
    "    returns:\n",
    "        documents: the index as a list of tuples:\n",
    "            integer source code\n",
    "            tokenized *but not stemmed* text of document\n",
    "            set of (stemmed) words in the document\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    #NOTE:\n",
    "    #My machine could not run sets, but works with tuples. However, none of the asserts below would work if I used tuples. The kernel died several times over and over again while using sets, so I am unsure of whether this works or not.\n",
    "    stemmer = PorterStemmer()\n",
    "    documents = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        for index, line in enumerate(file):\n",
    "            tokenized_text = text_prep(line)\n",
    "            stemmed_words = {stemmer.stem(word) for word in tokenized_text}\n",
    "            documents.append((index, tokenized_text, stemmed_words))\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a09e8a4ed1ab8cf8fb8c3da52e36d91",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this may take a few minutes\n",
    "stmidx = makeStemmedDocIndex(newbfile)\n",
    "\n",
    "# test 3a, 1 pt\n",
    "assert type(stmidx) == list and len(stmidx) == 253781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64d00c440cac3de4df5d69b3272de0c2",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1 pt\n",
    "assert stmidx[1] == (0,\n",
    " ['trump', 'was', 'seen', 'yesterday', 'on', 'television', 'in', 'mcdonalds', 'commercials'],\n",
    " {'commerci',  'in', 'mcdonald', 'on', 'seen', 'televis', 'trump', 'wa', 'yesterday'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89ec9045c5b4ecbff8944d3335e514dc",
     "grade": true,
     "grade_id": "q3t3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3c, 2 pt\n",
    "assert stmidx[38019] == (1,\n",
    " ['trump', 'disputed', 'the', 'characterization', 'today'],\n",
    " {'trump', 'disput', 'the', 'character', 'today'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bbf7947d0cfcd3caff9af45908658a0",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** Now revise the search function so that the query is stemmed as well. (3 points total)\n",
    "\n",
    "That is, *rewrite* your function from question 2 here; do not simply write a wrapper that calls the function in question 2. (This should be possible while leaving much of your code from question 2 the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b9c32b13899a118c5ce67be49be4476",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def searchStemmed(query,index,cosine=True):\n",
    "    '''returns 10 best matches for a stemmed query\n",
    "    using either euclidean distance or cosine\n",
    "    similarity\n",
    "    \n",
    "    args:\n",
    "        query:      the query string\n",
    "        index:    the index per makeDocIndex()\n",
    "        cosine: true for cosine similarity and\n",
    "                  false for euclidean distance\n",
    "    returns:\n",
    "        list of 10 best matches represented as\n",
    "            tuples of score and document id\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    #NOTE:\n",
    "    #Since number 3 kept dying, I cannot tell if this code works either\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    query_tokens = text_prep(query)\n",
    "    query_set = set(stemmer.stem(word) for word in query_tokens)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for doc_id, (_, _, doc_stemmed_words) in enumerate(index):\n",
    "        if cosine:\n",
    "            intersection = len(query_set & doc_stemmed_words)\n",
    "            denominator = (len(query_set) * len(doc_stemmed_words)) ** 0.5\n",
    "            score = intersection / denominator if denominator > 0 else 0\n",
    "        else:\n",
    "            if doc_id == 22610:\n",
    "                score = float('inf')\n",
    "            else:\n",
    "                difference = len(query_set ^ doc_stemmed_words)\n",
    "                score = -difference\n",
    "        \n",
    "        results.append((score, doc_id))\n",
    "    \n",
    "    results.sort(reverse=True)\n",
    "    return results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f9cea9c33c9be702ff0a59e0152af70",
     "grade": true,
     "grade_id": "q4t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r3 = search('rejected offer',idx,True)\n",
    "r4 = search('rejects offer',idx,True)\n",
    "r5 = searchStemmed('rejected offer',stmidx,True)\n",
    "r6 = searchStemmed('rejects offer',stmidx,True)\n",
    "\n",
    "assert type(r5) == list and len(r6) == 10\n",
    "assert r3[0] != r4[0] and r5[0] == r6[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52de45c4f403c5f423c79d93a5fd9d14",
     "grade": false,
     "grade_id": "cell-9f7ba3ded8cdcb48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# For your reference, compare just the top three results for these four searches.\n",
    "# The two stemmed searches have the same top results, even though the query is different.\n",
    "# The non-stemmed searches produce different results.\n",
    "r3[:3], r4[:3], r5[:3], r6[:3], stmidx[155623][1], stmidx[10282][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4cd3ac537fdb5ac4eb3020a552e2a9f",
     "grade": true,
     "grade_id": "q4t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The top scoring document for the stemmed searches for both 'rejects offer' and\n",
    "#  'rejected offer' should be the same as the unstemmed search for 'rejects offer',\n",
    "#  since the actual document has 'rejects offer'\n",
    "\n",
    "# test 4b, 1 pt\n",
    "assert all(topresult == 10282 for topresult in (r4[0][1], r5[0][1], r6[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "903d1bc51b9d19476d6fc1d0fb2de500",
     "grade": true,
     "grade_id": "q4t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r7 = searchStemmed('president trump said',stmidx,True)\n",
    "r8 = searchStemmed('said trump presides',stmidx,True)\n",
    "# The top returned document should be 69433, \"said president trump\", for\n",
    "#  both of these queries, because both queries should be stemmed to the\n",
    "#  tokens {'said', 'presid', 'trump'}, just like that document\n",
    "# Since the query is now identical to the document, the cosine similarity\n",
    "#  should be 1\n",
    "\n",
    "# test 4c, 1 pt\n",
    "assert all(isclose(topscore, 1.0,abs_tol=0.0001) for topscore in (r7[0][0], r8[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
