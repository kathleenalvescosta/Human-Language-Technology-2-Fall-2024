{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aab6315785a55821fd462327dd36765",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "Linguistics 531<br>\n",
    "Fall 2024<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted.\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. Don't use editors *other* than Jupyter Notebook to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates, or the work of someone on Stack Overflow, or the work of a generative AI model. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: Kathleen Costa\n",
    "\n",
    "people I talked to about the assignment: N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "925e619cf905b2f3c4a36f533e6322cb",
     "grade": false,
     "grade_id": "hw5header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #5\n",
    "\n",
    "**This is due Tuesday, November 19, 2024 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)).\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2d71de2120c8cd56b89bf0982e99765",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import isclose\n",
    "\n",
    "# Used in the cosimfreq() implementation from class:\n",
    "import numpy as np\n",
    "#  You're free to use it here in your functions, but it's not a requirement\n",
    "\n",
    "# As with last week, this might make your life much easier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f42544eac75a5b2661e9a9777dc7569",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "What I need on my machine to properly grade this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6533f7aefbac103db012fa4f2a36aa",
     "grade": false,
     "grade_id": "cell-ea976235a1443dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c5f3d329570f012501243607da41c1e",
     "grade": false,
     "grade_id": "cell-d62bb3f494727bb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*In the editable cell below, enter the path on your own machine,* then uncomment that line so the notebook works on your machine.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "newbfile = 'train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cec34f3bca69d6c2df01dc667f18ba75",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We're going to build a tf-idf document index from the first 50,000 lines of the `train_orig.txt` file. We will *not* stem or remove stop words.\n",
    "\n",
    "**1.** The first step is to build a term-document index and a document-term index. (8 points)\n",
    "\n",
    "Follow the guidelines in the docstring below and in the following assert statements. As with previous assignments, you'll create a text processing function as part of this, and when you read in the file, retain only upper and lower case ASCII letters, numbers, and the percent sign. Remember that the document ID is *not* the same as the publication source code.\n",
    "\n",
    "You should be able to re-use your code from previous assignments here. Note that what's new this week is (1) we've added a limit to how many lines of the input file we use, and (2) we're creating both a term-document index and a document-term index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53f71edfbd1e6f529af3e217d9420a96",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeIndices(filename,maxcount=50000):\n",
    "    '''create document and term indices for the NewB corpus\n",
    "    \n",
    "    args:\n",
    "        filename: location of the newB file\n",
    "        maxcount: maximum number of lines\n",
    "    \n",
    "    returns:\n",
    "        wordIndex: a dictionary from terms (as strings) to\n",
    "            a list of tuples of the form (doc-id, count)\n",
    "        docIndex: a list of tuples of the form\n",
    "           (publication code (as an integer),\n",
    "            text of document ie one sentence (as a list of strings),\n",
    "            document vector (as a set of tuples: (word,count))\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    wordIndex = {}  \n",
    "    docIndex = []   \n",
    "    docId = 0\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = [next(f) for _ in range(maxcount)]\n",
    "        \n",
    "        for line in lines:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "                \n",
    "            pubCode, text = int(parts[0]), parts[1]\n",
    "            tokens = text_prep(text)\n",
    "            \n",
    "            doc_vector = {}\n",
    "            for token in tokens:\n",
    "                doc_vector[token] = doc_vector.get(token, 0) + 1\n",
    "            \n",
    "            doc_vector_set = set((term, count) for term, count in doc_vector.items())\n",
    "            \n",
    "            for term, count in doc_vector.items():\n",
    "                if term not in wordIndex:\n",
    "                    wordIndex[term] = []\n",
    "                wordIndex[term].append((docId, count))\n",
    "            \n",
    "            docIndex.append((pubCode, tokens, doc_vector_set))\n",
    "            docId += 1\n",
    "    \n",
    "    return wordIndex, docIndex\n",
    "    \n",
    "def text_prep(input):\n",
    "    '''performs text normalization and tokenization on an input string\n",
    "    \n",
    "    Our process: anything that is not a letter (upper or lower case\n",
    "        ASCII letters), digit (0-9), or the percent sign (%) is converted\n",
    "        to space, and then terms are split on whitespace.\n",
    "    \n",
    "    args:\n",
    "        input: a string of unprocessed text\n",
    "    returns:\n",
    "        output: a list of strings of normalized tokens\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    text = re.sub(r'[^a-zA-Z0-9%]', ' ', input)\n",
    "    return [token.lower() for token in text.split() if token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835eda8681905bd706ca9a6e1c8a5769",
     "grade": false,
     "grade_id": "cell-fb03399e461fb809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You've probably had enough testing of your `text_prep()` function that it should be working fine now. I won't include separate testing here, but bear in mind that if you are having trouble passing the tests below, it may be due to a problem in your `text_prep()` function. If so, the \"practice\" tests for that function from Homework 4 may be useful to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "294a7ff3415360225c437f44c77fa505",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "widx,didx = makeIndices(newbfile)\n",
    "\n",
    "# test 1a, 1 pt\n",
    "assert type(widx) == dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79de7ff204a4a34907d541a296862c51",
     "grade": true,
     "grade_id": "q1t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1b, 1 pt\n",
    "# This one will be *very* sensitive to your text_prep() function\n",
    "assert len(widx) == 32204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "716fbefab3a2d3d4143e25b90faf94e1",
     "grade": true,
     "grade_id": "q1t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1c, 1 pt\n",
    "assert widx['apple'] == [(3984, 4),(10197, 1),(10604, 1),\n",
    " (27913, 1),(31235, 1),(33494, 1),(33511, 1),(46867, 1),\n",
    " (48326, 1),(48888, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbd8234e25dcaccae7486f29a175a6a4",
     "grade": true,
     "grade_id": "q1t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1d, 1 pt\n",
    "assert type(didx) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70492b6fbcf8a9802f11f1f739d3cb28",
     "grade": true,
     "grade_id": "q1t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1e, 1 pt\n",
    "assert len(didx) == 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f806316dbb12f7933d5ea06ae2141892",
     "grade": true,
     "grade_id": "q1t6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1f, 1 pt\n",
    "assert didx[30861][0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d998894ca58f1a727c84c00cd67a4f0a",
     "grade": true,
     "grade_id": "q1t7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1g, 1 pt\n",
    "assert didx[30861][1] == ['trump',  'holds',  'the',  'ground',  'lease',  'the',\n",
    "                          'lease',  'for',  'the',  'land',  'on',  'which',  'the',\n",
    "                          'building',  'stands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b354bdb3dc165a1010b14f6971263006",
     "grade": true,
     "grade_id": "q1t8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1h, 1 pt\n",
    "assert didx[30861][2] == {('building', 1),  ('for', 1),  ('ground', 1),  ('holds', 1),\n",
    "  ('land', 1),  ('lease', 2),  ('on', 1),  ('stands', 1),  ('the', 4),  ('trump', 1),\n",
    "  ('which', 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "771e2b07a1f3b137d3e6ce13f8d043e8",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.** Use the term-document index to calculate idf values (an intermediate step on the way to ***tf*-idf** values) and return a dictionary from terms to idf values. (4 points)\n",
    "\n",
    "Again, follow the guidance of the docstring and the following assert statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "222378d99b6ce3641bd3625f4b8c9dfa",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeIdfs(wordindex,maxcount=50000):\n",
    "    '''calculate the idf value for each term in a word index\n",
    "    \n",
    "    args:\n",
    "        wordindex: a term-document index as created by\n",
    "            makeIndices()\n",
    "        maxcount: total number of documents; by default,\n",
    "            this is 50000\n",
    "        \n",
    "    returns:\n",
    "        dictionary of each word in the collection\n",
    "            and its idf value\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    idfs = {}\n",
    "    \n",
    "    for term, doc_list in wordindex.items():\n",
    "        doc_count = len(doc_list)\n",
    "        idfs[term] = np.log10(maxcount / doc_count)\n",
    "        \n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5900e8dfbca175936d7bc06846953049",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "idfx = makeIdfs(widx)\n",
    "\n",
    "# test 2a, 1 pt\n",
    "assert type(idfx) == dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f383925f39c26b980e585beb2b78108",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1 pt\n",
    "# There should be an entry here for every term in your term-document index\n",
    "assert len(idfx) == 32204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bb6965b1b8e6eace5fbec570bd728ec",
     "grade": true,
     "grade_id": "q2t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2c, 1 pt\n",
    "# I get 4.698970004336019\n",
    "assert isclose(idfx['canon'],4.69897,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffd9a4cdb412a0519b3432a2f9841e63",
     "grade": true,
     "grade_id": "q2t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2d, 1 pt\n",
    "#I get 2.11690664142431\n",
    "assert isclose(idfx['help'],2.11690,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e13e48249fba46c4973e1aa82b768c3",
     "grade": false,
     "grade_id": "cell-36428267f135165f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we would expect, frequent terms (like function words) have very low idf values.\n",
    "\n",
    "Because of the nature of this collection, certain other terms also have very low idf scores, even though they're not function words or common stop words, and would in other collections likely have ***higher*** idf scores. (So, note that idf scores will *really* depend on your data set!)\n",
    "\n",
    "Here are the 20 terms with lowest idf scores; note that \"donald\" and \"trump\" are among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05ad65b9098a55ec9f5a623676758d8e",
     "grade": false,
     "grade_id": "cell-3b9efe21fb3a9664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 'trump'),\n",
       " (0.22883893956150084, 'the'),\n",
       " (0.3805727251185078, 'to'),\n",
       " (0.4324090650491771, 'a'),\n",
       " (0.450457272115119, 'and'),\n",
       " (0.45859550737778554, 'of'),\n",
       " (0.5042059802251323, 'in'),\n",
       " (0.6440124083236123, 'that'),\n",
       " (0.6946916321358564, 'his'),\n",
       " (0.7188302265902644, 'on'),\n",
       " (0.7319839792180809, 'for'),\n",
       " (0.770471736212328, 'donald'),\n",
       " (0.7723189272471299, 'he'),\n",
       " (0.8082137524178006, 'said'),\n",
       " (0.8260565627156234, 'is'),\n",
       " (0.835290336460121, 'with'),\n",
       " (0.9376436819145622, 'has'),\n",
       " (0.9437051129237721, 'was'),\n",
       " (0.9510146974292889, 'as'),\n",
       " (1.0061230850587888, 'at')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(idfx[key], key) for key in idfx])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2aaa130bc5cf240a39f5cea259a7684",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** We now build a tf-idf index from the document-term index and the idf values. (6 points)\n",
    "\n",
    "*HINT: This is really an index with precisely the same structure as our document-term index, but simply with different numerical values--floats, not just integer counts--in each document vector.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "089c08eefa854d851b67070aa90f46af",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeTfidf(idfs,docindex):\n",
    "    '''creates a tfidf document-term index from idf scores\n",
    "    and a document index\n",
    "    \n",
    "    args:\n",
    "        idfs: idf scores (as a dictionary) as\n",
    "            created by makeIdfs()\n",
    "        docindex: a document index as created by\n",
    "            makeIndices()\n",
    "    returns:\n",
    "        tf-idf index as a list where the list\n",
    "            index corresponds to the docID\n",
    "            and each entry is a triple:\n",
    "                publication code\n",
    "                document text\n",
    "                set of pairs:\n",
    "                    word\n",
    "                    tfidf score\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    tfidf_index = []\n",
    "    \n",
    "    for doc_id, (pub_code, doc_text, term_counts) in enumerate(docindex):\n",
    "        tfidf_scores = set()\n",
    "        for term, count in term_counts:\n",
    "            tfidf = count * idfs[term]\n",
    "            tfidf_scores.add((term, tfidf))\n",
    "        \n",
    "        tfidf_index.append((pub_code, doc_text, tfidf_scores))\n",
    "    \n",
    "    return tfidf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1d9e4de5e86ce030afed3cf0c31d2d",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tfidfidx = makeTfidf(idfx,didx)\n",
    "\n",
    "# test 3a, 1 pt\n",
    "assert type(tfidfidx) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1e7df106069bbbc348634c2bf8d6cb9",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1 pt\n",
    "assert len(tfidfidx) == 50000 and tfidfidx[49000][0] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62072668a6c31f32c583be11092ef1da",
     "grade": true,
     "grade_id": "q3t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3c, 1 pt\n",
    "assert tfidfidx[49000][1] == ['donald', 'trump', 'didnt', 'tell', 'the', 'truth', '83', 'times', 'in', '1', 'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14e0cbdf2e02609722f8fb5adb373fed",
     "grade": false,
     "grade_id": "cell-b27bfcd6ea8e3f02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      " ['the', 'copying', 'of', 'the', 'letter', 'to', 'the', 'justice', 'department', 'attracted', 'wide', 'notice', 'in', 'washington', 's', 'close', 'knit', 'election', 'law', 'bar', 'as', 'did', 'the', 'claim', 'in', 'the', 'lawsuit', 'that', 'the', 'use', 'of', 'the', 'trump', 'foundation', 'to', 'benefit', 'the', 'trump', 'campaign', 'was', 'willful', 'and', 'knowing'] \n",
      "\n",
      "Raw counts:\n",
      " [('and', 1), ('as', 1), ('attracted', 1), ('bar', 1), ('benefit', 1), ('campaign', 1), ('claim', 1), ('close', 1), ('copying', 1), ('department', 1), ('did', 1), ('election', 1), ('foundation', 1), ('in', 2), ('justice', 1), ('knit', 1), ('knowing', 1), ('law', 1), ('lawsuit', 1), ('letter', 1), ('notice', 1), ('of', 2), ('s', 1), ('that', 1), ('the', 8), ('to', 2), ('trump', 2), ('use', 1), ('was', 1), ('washington', 1), ('wide', 1), ('willful', 1)] \n",
      "\n",
      "tfidf:\n",
      " [('and', 0.450457272115119), ('as', 0.9510146974292889), ('attracted', 3.3767507096020997), ('bar', 3.0087739243075053), ('benefit', 2.703334809738469), ('campaign', 1.322393047279507), ('claim', 2.503070351926785), ('close', 2.2104192878355744), ('copying', 4.3979400086720375), ('department', 2.157390760389438), ('did', 1.661942124580244), ('election', 1.6955094722265123), ('foundation', 2.46344155742847), ('in', 1.0084119604502646), ('justice', 2.2549252084179425), ('knit', 4.3979400086720375), ('knowing', 3.2518119729937998), ('law', 2.141462802430361), ('lawsuit', 2.4509967379742124), ('letter', 2.417936637088291), ('notice', 3.065501548756432), ('of', 0.9171910147555711), ('s', 1.556893543262734), ('that', 0.6440124083236123), ('the', 1.8307115164920067), ('to', 0.7611454502370156), ('trump', 0.0), ('use', 2.175223537524454), ('was', 0.9437051129237721), ('washington', 1.7539940959239708), ('wide', 3.1079053973095196), ('willful', 4.221848749616356)]\n"
     ]
    }
   ],
   "source": [
    "# Here's a document we'll look at\n",
    "print(\"Sentence:\\n\",tfidfidx[41584][1],'\\n')\n",
    "print(\"Raw counts:\\n\",sorted(didx[41584][2]),'\\n')\n",
    "print(\"tfidf:\\n\",sorted(tfidfidx[41584][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1aa1aff192d898104c02bc265bbeedfd",
     "grade": true,
     "grade_id": "q3t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# I'm going to put the tf-idf representation of the document into a Counter so it's\n",
    "#  easier to pull values out for a specific term (using the Counter like a dict)\n",
    "res = Counter(dict(tfidfidx[41584][2]))\n",
    "\n",
    "# test 3d, 1 pt\n",
    "# I get 0 and 4.221848749616356\n",
    "assert isclose(res['trump'],0.,abs_tol=0.0001) and isclose(res['willful'],4.2218,abs_tol=0.0001)\n",
    "# I get 0.9171910147555711 and 1.8307115164920067\n",
    "assert isclose(res['of'],0.91719,abs_tol=0.0001) and isclose(res['the'],1.83071,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87d6f3dc204a26956ee4168e5ed4bf6a",
     "grade": false,
     "grade_id": "cell-24243588e6db2883",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['journalism', 'somehow', 'managed', 'to', 'get', 'the', 'trump', 'phenomenon', 'wrong', 'wrong', 'wrong', 'while', 'writing', 'more', 'and', 'more', 'about', 'trump', 'trump', 'trump']\n"
     ]
    }
   ],
   "source": [
    "# Here's another document that we'll look at\n",
    "print(didx[43272][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19fc386b6c46a2889565438281ceaa36",
     "grade": true,
     "grade_id": "q3t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res = Counter(dict(tfidfidx[43272][2]))\n",
    "\n",
    "# test 3e, 1 pt\n",
    "# I get 2.8459360287479374 and 7.517536217944672\n",
    "assert isclose(res['more'],2.8459,abs_tol=0.0001) and isclose(res['wrong'],7.5175,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea34d8e47d10907440bc784ef07b8a22",
     "grade": false,
     "grade_id": "cell-cc0fa5180236959b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'when', 'you', 'have', 'a', 'fight', 'you', 'get', 'sinatra', 'you', 'get', 'donald', 'trump', 'you', 'get', 'lee', 'iacocca', 'and', 'you', 'get', 'dr']\n"
     ]
    }
   ],
   "source": [
    "# Here's another document that we'll look at\n",
    "print(didx[7328][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4e1cc6c1d874f001d256549391e8898",
     "grade": true,
     "grade_id": "q3t6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res = Counter(dict(tfidfidx[7328][2]))\n",
    "\n",
    "# test 3f, 1 pt\n",
    "# I get 8.040034638701437 and 7.415487857287048\n",
    "assert isclose(res['you'],8.0400,abs_tol=0.0001) and isclose(res['get'],7.4155,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "068680b25e6eed58fd567bfc572a229d",
     "grade": false,
     "grade_id": "cosimfreq1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll need the `cosimfreq()` function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dca9c51825206a469288094833a108d",
     "grade": false,
     "grade_id": "cosimfreq2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#cosine similarity wrt/frequencies or tfidfs\n",
    "def cdot(count1, count2):\n",
    "    \"\"\"dot product for two vectors as Counters\"\"\"\n",
    "    return sum(count1[word]*count2[word] for word in count1)\n",
    "\n",
    "def clength(count1):\n",
    "    \"\"\"length of a vector as Counter\"\"\"\n",
    "    return np.sqrt(sum(count1[word]**2 for word in count1))\n",
    "\n",
    "def cosimfreq(d1,d2):\n",
    "    # gotta get 'em from lists of tuples into Counters\n",
    "    d1, d2 = Counter(dict(d1)), Counter(dict(d2))\n",
    "    d1len, d2len = clength(d1), clength(d2)\n",
    "    denom = d1len * d2len\n",
    "    if denom == 0: return 0\n",
    "    return float(cdot(d1, d2))/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b06fc65d8cd024a2349271e032fa748d",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** Write a search function for the tf-idf index from question 3 that uses only cosine similarity (as given just above). (5 points)\n",
    "\n",
    "The function should have this argument structure:\n",
    "\n",
    "```python\n",
    "search(query, index, idfs)\n",
    "```\n",
    "\n",
    "The function should return a rank-ordered list of the 10 **best** matches.\n",
    "\n",
    "*Hint: Note that the idf scores, as well as the tf-idf index and query, are being passed to this search function. Consider the last several homework assignments: often you made a change to the structure of an index, which meant that you needed to make the same change to the distance functions and to the search function. What did you have to update in the search functions before?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e479934dfaf8c152eb667ac6e84556b",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def search(query,index,idfs):\n",
    "    '''search a tf-idf index for documents that are most similar\n",
    "       to a query\n",
    "       (This time we won't offer a choice of distance function,\n",
    "       but will just use cosine similarity.)\n",
    "    \n",
    "    args:\n",
    "        query: search query (as a string)\n",
    "        idx: tf-idf index (as a list of tuples)\n",
    "        idfs: idf scores (as a dictionary)\n",
    "    returns:\n",
    "        10 best matches as a list of tuples:\n",
    "            cosine similarity score\n",
    "            document index\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    query_terms = re.sub(r'[^a-zA-Z0-9%]', ' ', query.lower()).split()\n",
    "    \n",
    "    query_counts = Counter(query_terms)\n",
    "    query_vector = {}\n",
    "    for term in query_terms:\n",
    "        if term in idfs:\n",
    "            query_vector[term] = query_counts[term] * idfs[term]\n",
    "    \n",
    "    results = []\n",
    "    for doc_id, (_, _, doc_vector) in enumerate(index):\n",
    "        doc_dict = dict(doc_vector)\n",
    "        \n",
    "        numerator = sum(query_vector.get(term, 0.0) * doc_dict.get(term, 0.0) \n",
    "                       for term in set(query_vector) | set(doc_dict))\n",
    "        \n",
    "        query_magnitude = np.sqrt(sum(v * v for v in query_vector.values()))\n",
    "        doc_magnitude = np.sqrt(sum(v * v for v in doc_dict.values()))\n",
    "        \n",
    "        if query_magnitude and doc_magnitude:\n",
    "            similarity = float(numerator / (query_magnitude * doc_magnitude))\n",
    "            results.append((similarity, doc_id))\n",
    "        else:\n",
    "            results.append((0.0, doc_id))\n",
    "    \n",
    "    return sorted(results, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dba3d77bc1723ac74c977616713546ac",
     "grade": true,
     "grade_id": "q4t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res1 = search(\"melania is trump's wife\",tfidfidx,idfx)\n",
    "\n",
    "# test 4a, 1 pt\n",
    "# Make sure you're getting the structure right\n",
    "assert type(res1) == list and type(res1[0]) == tuple and type(res1[0][0]) == float and type(res1[0][1]) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bce074be334502595f9e5996feaa0fb1",
     "grade": true,
     "grade_id": "q4t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4b, 1 pt\n",
    "# Make sure you're getting the right results returned\n",
    "assert res1[0][1] == 29285 and res1[1][1] == 41827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fb71fc3ca7fd5fd44cd784f512b2ccc",
     "grade": false,
     "grade_id": "cell-f255ffb79bcd34d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0.849969028092293, 29285), 'trump and his wife melania'),\n",
       " ((0.6215127702787681, 41827), 'trump and his wife melania hosted mr'),\n",
       " ((0.5183980745617279, 46573),\n",
       "  'full text here is a look at the life of melania trump wife of president donald trump'),\n",
       " ((0.5083112011174594, 21175),\n",
       "  'trump is 24 years older than his wife melania'),\n",
       " ((0.5043656908492963, 10060),\n",
       "  'from left trump his son barron trump and wife melania trump'),\n",
       " ((0.5013248622362927, 32775),\n",
       "  'trump and his wife melania who was pregnant at the time'),\n",
       " ((0.4673037889163044, 15962),\n",
       "  'trump was accompanied by his wife melania who introduced him'),\n",
       " ((0.45156314335095366, 21510),\n",
       "  'he then introduced his wife melania who said trump will work for all americans'),\n",
       " ((0.45156314335095366, 3499),\n",
       "  'he then introduced his wife melania who said trump will work for all americans'),\n",
       " ((0.4364630422890691, 47305),\n",
       "  'full text here is a look at the life of melania trump wife of 45th us president donald trump')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a sanity check, here are your results:\n",
    "[(result,' '.join(didx[result[1]][1])) for result in res1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1f5c26b47c8ec875d16734cdd3a0564",
     "grade": true,
     "grade_id": "q4t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4c, 1 pt\n",
    "# Make sure you're getting the right similarity values, too\n",
    "# For the top two, I get 0.849969028092293 and 0.6215127702787681\n",
    "assert isclose(res1[0][0],0.84997,abs_tol=0.0001) and isclose(res1[1][0],0.62151,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "760fcfa1e6381e784683c669b87461dd",
     "grade": true,
     "grade_id": "q4t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res2 = search('trump lives in the white house',tfidfidx,idfx)\n",
    "\n",
    "# test 4d, 1 pt\n",
    "assert res2[0][1] == 22752 and res2[4][1] == 45145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4389896027412c1bc0ba43da49745afa",
     "grade": false,
     "grade_id": "cell-a0efd8cb77ccc097",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0.6319877962507022, 22752), 'trump in the white house'),\n",
       " ((0.6158608687047019, 11090), 'the trump white house'),\n",
       " ((0.6067586288920347, 31782), 'trump to the white house'),\n",
       " ((0.5931508697856988, 37704), 'trump is in the white house'),\n",
       " ((0.49816997165817267, 45145), 'trump and the white house said mr'),\n",
       " ((0.4539447710242628, 13798), 'full text the trump white house'),\n",
       " ((0.42225789139265635, 15188), 'he lives on the phone walker said of trump'),\n",
       " ((0.40932614931370964, 2490), 'we can go on with our lives ivana trump said'),\n",
       " ((0.4027002229591978, 6749), 'one of the characters lives in trump towers'),\n",
       " ((0.39402213680464887, 26481), 'trump was in the white house for a reason')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "[(result,' '.join(didx[result[1]][1])) for result in res2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f71b844064ae4fed4fdbde4dc7f41726",
     "grade": true,
     "grade_id": "q4t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4e, 1 pt\n",
    "# I get 0.3940221368046489 for the tenth result\n",
    "assert isclose(res2[9][0],0.3940, abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c58e2f3d06e444468a263d5d1cbeb4a4",
     "grade": false,
     "grade_id": "q5q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.** Calculate the MAP score for your search function based on `res1` and `res2` above *by hand* (that is, step by step, typed below) and *show your work*. (2 points)\n",
    "\n",
    "Note that this requires that you look up the sentence for each returned result and judge whether it was relevant to the query. Your choice of relevant or not isn't graded, so don't invest too much thought in what *ought* to be relevant or not; choose a judgment for each returned result, and then you'll need to properly calculate the MAP score based on those choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1df20226606a10244b111a9c2ec5e1b",
     "grade": false,
     "grade_id": "q5q-resource",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, here is some text formatting code in Markdown for you to use. Copy the source of this cell into the answer cell (since this cell will be read-only), then give your relevance scores in this table. Code each result as \"relevant\" (Y) or \"not relevant\" (N) for its query&mdash;meaning, is this result a good result given that particular query.\n",
    "\n",
    "As an example of what it means for one of our results to be \"relevant\" for the queries here, consider our `res2` query, \"trump lives in the white house\". Many of the results involve the terms \"trump\" and \"white house\", and to me they do seem like reasonable results for that query, but note that result 15188 is \"he lives on the phone walker said of trump\". To me, that seems like **not** a relevant result for \"trump lives in the white house\". However, the final decision is up to you, and I'm the one who has to make sure that the math is correct given *your* relevancy results.\n",
    "\n",
    "&nbsp; | `res1` | `res2`\n",
    "-------|--------|-------\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "\n",
    "Based on your table, once you generate it in the cell below, please show the step-by-step calculation of the MAP score for this search system. You may find the following references helpful if you need to properly format any of your mathematical equations in LaTeX and Markdown:\n",
    "\n",
    "https://jupyterbook.org/en/stable/content/math.html\n",
    "\n",
    "https://ashki23.github.io/markdown-latex.html#latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ea83055d48ffa3375a5880ffa705423",
     "grade": true,
     "grade_id": "q5a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; | `res1` | `res2`\n",
    "-------|--------|-------\n",
    "0      |   Y    |   Y\n",
    "1      |   Y    |   Y\n",
    "2      |   Y    |   Y\n",
    "3      |   Y    |   Y\n",
    "4      |   Y    |   N\n",
    "5      |   Y    |   N\n",
    "6      |   Y    |   N\n",
    "7      |   Y    |   N\n",
    "8      |   Y    |   N\n",
    "9      |   Y    |   Y\n",
    "\n",
    "For res1, since all the results were relevant, we will calculate the precision of the relevant lines: \n",
    "$$\n",
    "\\mathrm{AP}(q) = \\frac{1}{m} \\sum_{k=1}^{m} \\mathrm{Precision}(R_k)\n",
    "$$\n",
    "Here, the number of relevant documents I found was 10, and that is the overall number of documents present so the calculation for res1\n",
    "would be:\n",
    "   - Rank 1: $$ \\text{Precision} = \\frac{1}{1} = 1.0  $$ \n",
    "   - Rank 2: $$ \\text{Precision} = \\frac{2}{2} = 1.0 $$  \n",
    "   - Rank 3: $$ \\text{Precision} = \\frac{3}{3} = 1.0 $$  \n",
    "   - Rank 4: $$ \\text{Precision} = \\frac{4}{4} = 1.0 $$  \n",
    "   - Rank 5: $$ \\text{Precision} = \\frac{5}{5} = 1.0 $$  \n",
    "   - Rank 6: $$ \\text{Precision} = \\frac{6}{6} = 1.0 $$  \n",
    "   - Rank 7: $$ \\text{Precision} = \\frac{7}{7} = 1.0 $$  \n",
    "   - Rank 8: $$ \\text{Precision} = \\frac{8}{8} = 1.0 $$  \n",
    "   - Rank 9: $$ \\text{Precision} = \\frac{9}{9} = 1.0 $$  \n",
    "   - Rank 10: $$ \\text{Precision} = \\frac{10}{10} = 1.0 $$  \n",
    "\n",
    "\n",
    "$$\n",
    "\\text{AP} = \\frac{1.0+1.0+1.0+1.0+1.0+1.0+1.0+1.0+1.0+1.0=}{10} = 1.0\n",
    "$$\n",
    "\n",
    "The reason I chose all the documents as relevant here is because they all mention Trump, Melania, and the word \"wife\" to describe Melania. \n",
    "\n",
    "\n",
    "Using the same equation for res2, the calculation of the precision of relevant lines is:\n",
    "   - Rank 1: $$ \\text{Precision} = \\frac{1}{1} = 1.0 $$  \n",
    "   - Rank 2: $$ \\text{Precision} = \\frac{2}{2} = 1.0 $$  \n",
    "   - Rank 3: $$ \\text{Precision} = \\frac{3}{3} = 1.0 $$  \n",
    "   - Rank 4: $$ \\text{Precision} = \\frac{4}{4} = 1.0 $$  \n",
    "   - Rank 10: $$ \\text{Precision} = \\frac{5}{10} = 0.5 $$  \n",
    "\n",
    "\n",
    "$$\n",
    "\\text{AP} = \\frac{1.0+1.0+1.0+1.0+0.5=}{5} = 0.9 \n",
    "$$\n",
    "\n",
    "The reason I chose these specific documents as relevant is because while some documents do mention Trump in the White House, some of the\n",
    "results were irrelevant and discussed other members of the Trump family or other locations that the Trumps might frequent.\n",
    "\n",
    "To calculate the overall MAP, we will need to take an average of the MAPs for both results. That would look like:\n",
    "$$\n",
    "\\text{MAP} = \\frac{\\text{AP(res1)} + \\text{AP(res2)}}{2} = \\frac{1.0 + 0.9}{2} = 0.95\n",
    "$$\n",
    "\n",
    "Overall, the precision of this model is 0.95. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
