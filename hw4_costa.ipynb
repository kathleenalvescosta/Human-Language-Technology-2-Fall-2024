{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aab6315785a55821fd462327dd36765",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "Linguistics 531<br>\n",
    "Fall 2024<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted.\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. Don't use editors *other* than Jupyter Notebook to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates, or the work of someone on Stack Overflow, or the work of a generative AI model. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: *\\<FILL IN HERE\\>*\n",
    "\n",
    "people I talked to about the assignment: *\\<FILL IN HERE\\>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "843a187219cd578be3dd9fad7bc82a79",
     "grade": false,
     "grade_id": "hw4header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #4\n",
    "\n",
    "**This is due Tuesday, November 12, 2024 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)).\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a011039658a7c8780f41fbca9f8d11ae",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import isclose\n",
    "\n",
    "# Used in the cosimfreq() implementation from class\n",
    "import numpy as np\n",
    "\n",
    "# HINT: This might make your life much easier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e94107167185f141e929cb6dcbf0f10",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "Again, for grading, I need to be working with the right file that we load our corpus from. On my machine, that file has this path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28497011f8bd6b55ec508bc5635b041a",
     "grade": false,
     "grade_id": "cell-b7e8e00e5c53b643",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a5b37d970ffefc8e8fb9ed52a2f6b7f",
     "grade": false,
     "grade_id": "cell-26968d9281db051c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "For **you** to work on your own code, you need to point this notebook to the path for this file on your own machine. *You should enter the path on your own machine in the editable code cell below,* then uncomment that line so the notebook works on your machine. This means that the second code cell will take precedence in assigning the value of the path to the corpus, and you can write your code to open that file without problems.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.** This means that when I run the code on my own machine, it'll have the path that ***I*** need, and it'll grade your notebook properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "newbfile = 'train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df081de361bc4cec769b94a928d9e16d",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.** Build a frequency-based *document* index from the `train_orig.txt` file. (6 points total)\n",
    "\n",
    "You may adapt any of the code from class for this or use your own. You may also use or adapt the code from your previous assignments. Do *not* stem or remove stop words, but *do* use the processing that we've done before: leave only upper and lower case ASCII letters, and numbers. We'll continue the practice from last week of putting our text normalization into a separate function, `text_prep()`, so that we can easily do the same processing for our collection and for our queries.\n",
    "\n",
    "Your `makeFreqIdx()` function should return a list where the positional index of each item (ie, what you might get from `enumerate`-ing the document index) corresponds to the document ID. The value of each item in the list is a tuple composed of: i) the publication source code for that document, as an integer; ii) the text of that document, normalized and tokenized, as a list of strings; and iii) a set that contains tuples of terms and counts—a word (a normalized string) and a count (an integer) of how many times it occurs in this document.\n",
    "\n",
    "You should see that this is really quite similar to what you wrote last week. The only difference comes in the set that is the third element of each document in our collection. Last week, that set was composed of just terms. This week, that set is composed of tuples of terms and counts. Now that we're counting terms, do you think a `Counter()` object would help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdfcf13435b987b85729504949c20ee9",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeFreqIdx(filename):\n",
    "    '''create a frequency-based document index from\n",
    "    the newB source file\n",
    "    \n",
    "    args:\n",
    "        filename: name/location of train_orig.txt\n",
    "    returns:\n",
    "        a frequency-based document index represented\n",
    "            as a list of tuples:\n",
    "                publication code, as an integer\n",
    "                text of document (ie, one sentence), as a list of strings\n",
    "                set of count tuples: (word, count)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    doc_index = []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                source_code = int(parts[0])\n",
    "                text = parts[1]\n",
    "                tokens = text_prep(text)\n",
    "                freq_counts = Counter(tokens)\n",
    "                freq_set = {(word, count) for word, count in freq_counts.items()}\n",
    "                doc_index.append((source_code, tokens, freq_set))\n",
    "    \n",
    "    return doc_index\n",
    "def text_prep(input):\n",
    "    '''performs text normalization and tokenization on an input string\n",
    "    \n",
    "    Our process: anything that is not a letter (upper or lower case\n",
    "        ASCII letters), digit (0-9), or the percent sign (%) is converted\n",
    "        to space, and then terms are split on whitespace.\n",
    "    \n",
    "    args:\n",
    "        input: a string of unprocessed text\n",
    "    returns:\n",
    "        output: a list of strings of normalized tokens\n",
    "    '''\n",
    "    # NOTE: This time, we're not returning both a normalized string AND\n",
    "    #       a tokenized list of strings--just the tokenized list of strings.\n",
    "    # YOUR CODE HERE\n",
    "    normalized = re.sub(r'[^a-zA-Z0-9]', ' ', input)\n",
    "    return normalized.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49492ce6f35df0408c7ffdaa1946e6f1",
     "grade": false,
     "grade_id": "cell-14d9477f348b4416",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Here's a bit of a test for your `text_prep()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7617881909076c9eeaa114da7d02f30",
     "grade": false,
     "grade_id": "cell-ea0aadb36a1b3934",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  ['this', 'is', 'a', 'weird', 'string']\n",
      " Yours:  ['this', 'is', 'a', 'weird', 'string']\n",
      "Result:  Matches! \n",
      "\n",
      "Target:  ['this', 'string', 'is', '98%', 'oDd']\n",
      " Yours:  ['this', 'string', 'is', '98', 'oDd']\n",
      "Result:  Doesn't match! \n",
      "\n",
      "Target:  ['now', 'we', 'have', 'some', 'W', 'I', 'D', 'characters']\n",
      " Yours:  ['now', 'we', 'have', 'some', 'W', 'I', 'D', 'characters']\n",
      "Result:  Matches! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some sample input\n",
    "test1 = 'this is a_weird string  \\n'\n",
    "test2 = 'this string is 98% oDd.'\n",
    "test3 = 'now we have some WĔIřD•characters'\n",
    "tests = [test1, test2, test3]\n",
    "\n",
    "#pay attention to how this should be tokenized according to the instructions\n",
    "target1 = ['this', 'is', 'a', 'weird', 'string']\n",
    "target2 = ['this', 'string', 'is', '98%', 'oDd']\n",
    "target3 = ['now', 'we', 'have', 'some', 'W', 'I', 'D', 'characters']\n",
    "targets = [target1, target2, target3]\n",
    "\n",
    "for test, target in zip(tests, targets):\n",
    "    yourout = text_prep(test)\n",
    "    result = \"Matches!\" if target == yourout else \"Doesn't match!\"\n",
    "    print(\"Target: \",target)\n",
    "    print(\" Yours: \",yourout)\n",
    "    print(\"Result: \", result, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84e88bbef6981cfc718c4925babc2547",
     "grade": false,
     "grade_id": "cell-b204489d35ff03fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'this': 1, 'string': 1, 'is': 1, '98%': 1, 'oDd': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of working with a Counter()\n",
    "count2 = Counter(target2)\n",
    "count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07ba0aec542494ede4ca2aa9efa1e0a7",
     "grade": false,
     "grade_id": "cell-e770c8bbf6d64c2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('98%', 1), ('is', 1), ('oDd', 1), ('string', 1), ('this', 1)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a way to get the Counter() to give you a set of (key, value) tuples\n",
    "#  (the set() function is there just to get the types right)\n",
    "set(count2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdb6b36f0fe94d44e66c463b23fd0887",
     "grade": false,
     "grade_id": "cell-bc0564060a5ff503",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's how to get something from a set of tuples back into a Counter()\n",
    "mytuples = {('98%', 1), ('is', 1), ('oDd', 1), ('string', 1), ('this', 1)}\n",
    "\n",
    "# The trick is to convert this set of tuples into a dictionary, and from\n",
    "#   there easily into a Counter()\n",
    "newcounter = Counter(dict(mytuples))\n",
    "\n",
    "#Is this the same as what we started with?\n",
    "newcounter == count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "235e22215b71662f5fce01e983936567",
     "grade": false,
     "grade_id": "cell-20d8ab566b2d4c25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "One of the nice things about `Counter()` objects is that they don't complain if you ask for a key that they don't have; they simply return a value of zero (ie, they didn't count that key when they were created). This can make for some Python code that is MUCH easier to read! Consider this example of a dot product function on `Counter()` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72d1842756178242e14fc1304070c4bb",
     "grade": false,
     "grade_id": "cell-c3d90b5b77264c8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cdot(counter1, counter2):\n",
    "    '''calculate the dot product over two counters'''\n",
    "    return sum(counter1[word]*counter2[word] for word in counter1)\n",
    "    # Think about why I only have to iterate over one of the counters for this calculation\n",
    "    # Could you also write a function to find the \"vector magnitude\" of a Counter()?\n",
    "    # With those two functions, could you also rewrite cosim() in terms of Counter()s?\n",
    "\n",
    "count1, count2 = Counter(target1), Counter(target2)\n",
    "cdot(count1, count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ced3fe123f1009a90a4aef334cbe94de",
     "grade": false,
     "grade_id": "cell-b0804ca5b161f088",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Tests for Q1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27501501183bf2f9e6baca1b30ad5f53",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#this will take a few seconds\n",
    "docs = makeFreqIdx(newbfile)\n",
    "\n",
    "# test 1a, 1 pt\n",
    "assert type(docs) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2faab83a17fef522c45875084122f7bf",
     "grade": true,
     "grade_id": "q1t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1b, 1 pt\n",
    "assert len(docs) == 253781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "850adc06e589e9661d3bc9b2c3b904c4",
     "grade": true,
     "grade_id": "q1t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1c, 1 pt\n",
    "assert type(docs[0]) == tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7b0fcfd6ab3b8bb79f26d4a1ffa83ee",
     "grade": true,
     "grade_id": "q1t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1d, 1 pt\n",
    "assert type(docs[500][0]) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9926ca397ac0ee01f3b88054f6fa556",
     "grade": true,
     "grade_id": "q1t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1e, 1 pt\n",
    "assert docs[10000][1] == ['the', 'snack', 'bar', 'across', 'the', 'mall',\n",
    "                          'from', 'the', 'trump', 'on', 'the', 'ocean',\n",
    "                          'site', 'was', 'badly', 'flooded', 'and', 'damaged',\n",
    "                          'even', 'though', 'it', 'has', 'no', 'basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36d25afe905394c518112274e11eb328",
     "grade": true,
     "grade_id": "q1t6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1f, 1 pt\n",
    "assert docs[38032][2] == {('a', 2), ('agreed', 1), ('and', 2), ('as', 2),\n",
    " ('asked', 1), ('be', 2), ('but', 1), ('by', 1), ('did', 1), ('even', 1),\n",
    " ('far', 1), ('fear', 1), ('fed', 3), ('from', 1), ('government', 1), ('im', 3),\n",
    " ('in', 1), ('is', 1), ('known', 1), ('lives', 1), ('lumber', 1), ('name', 1),\n",
    " ('named', 1), ('not', 2), ('of', 1), ('others', 1), ('partisan', 1), ('quoted', 1),\n",
    " ('rancor', 1), ('reflecting', 1), ('regulation', 1), ('said', 1), ('square', 1),\n",
    " ('stronghold', 1), ('taxes', 1), ('the', 1), ('then', 1), ('to', 2), ('trump', 2),\n",
    " ('up', 3), ('what', 1), ('who', 2), ('wholesaler', 1), ('with', 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aa8bfa2c1d5221307a1220136ac57fb",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2**. Revise our Euclidean distance function to operate over these new index structures, now that we're using frequency counts. (4 points total)\n",
    "\n",
    "Note, this means that we need to adjust that function since we're not just looking at term incidence, where all weights effectively equal 1. Be careful; this can be tricky! There are different ways to approach it, and some are faster than others.\n",
    "\n",
    "*(Hint: What are the different cases of values in the two vectors that you have to calculate for this? How much of the code from the in-class notebook—both from last week's Euclidean distance function that worked on unweighted vectors, and this week's cosine similarity function that did work on weighted vectors—can you re-use? Here's one place where using a Counter() may give you some helpful default behavior. Pay attention to the TYPES that this function expects--you can't just pass it a Counter(), but you might be able to convert a set of tuples into a Counter() pretty easily, rigiht?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80c104d9e51c01cb0ae8e351a47d8e6b",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def eucdistfreq(d1,d2):\n",
    "    '''calculate Euclidean distance for frequency-based\n",
    "    document representations as produced by makeFreqIdx()\n",
    "    \n",
    "    args:\n",
    "        d1,d2: document vectors represented as sets of\n",
    "           tuples of the form (word,count)\n",
    "    returns:\n",
    "        Euclidean distance between the two document vectors\n",
    "        (return a scalar of type float to pass the following assert test)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    v1 = Counter(dict(d1))\n",
    "    v2 = Counter(dict(d2))\n",
    "    \n",
    "    all_terms = set(v1.keys()) | set(v2.keys())\n",
    "    squared_diff_sum = sum((v1[term] - v2[term]) ** 2 for term in all_terms)\n",
    "    return float(np.sqrt(squared_diff_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db9169798a45bfe20a9df24e1075cb99",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's get two documents to use\n",
    "d0 = docs[0][2]\n",
    "d8 = docs[8][2]\n",
    "\n",
    "# The order of these vectors shouldn't matter\n",
    "res1 = eucdistfreq(d0,d8)\n",
    "res2 = eucdistfreq(d8,d0)\n",
    "\n",
    "# test 2a, 1pt\n",
    "assert type(res1) == float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58c7f6541bbe0ca527efaafcfbf59d64",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1pt\n",
    "# The order that the vectors are fed to eucdistfreq shouldn't change the result\n",
    "assert res1 == res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "214f0313686f1c7bb2bb93c0f374e136",
     "grade": true,
     "grade_id": "q2t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2c, 1pt\n",
    "# I get a value 5.477225575051661\n",
    "assert isclose(res1,5.4772,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8a9e84a7b87ebd8d2e9bf841a601f60",
     "grade": true,
     "grade_id": "q2t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d2 = docs[2][2]\n",
    "res3 = eucdistfreq(d8,d2)\n",
    "\n",
    "# test 2d, 1pt\n",
    "# I get a value 7.0710678118654755\n",
    "assert isclose(res3,7.0711,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c8283836adca0217e6ccb8f0f3c890f",
     "grade": false,
     "grade_id": "fromclass1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll also need the `cosimfreq()` function from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6501c3460e863096564893f67f9859e1",
     "grade": false,
     "grade_id": "fromclass2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#cosine similarity wrt/frequencies for DTI (from class)\n",
    "def cosimfreq(d1,d2):\n",
    "    num = sum([e1[1]*e2[1] for e1 in d1\n",
    "               for e2 in d2 if e1[0] == e2[0]])\n",
    "    d1len = np.sqrt(sum([e[1]**2 for e in d1]))\n",
    "    d2len = np.sqrt(sum([e[1]**2 for e in d2]))\n",
    "    denom = d1len * d2len\n",
    "    if denom == 0: return 0\n",
    "    return float(num)/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54235c0b79f384ecbb193acd394a4e7a",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** Revise your search function from last time so that it works with weighted document vectors and query vectors. (5 points)\n",
    "\n",
    "**It should process the query just like you processed your documents,** and then return the top 10 document indices that best match a query using either Euclidean distance or cosine similarity. (Remember that sorting should depend on the distance metric that is used.)\n",
    "\n",
    "The function should have this argument structure:\n",
    "\n",
    "```python\n",
    "search(query,index,cosine=True)\n",
    "```\n",
    "\n",
    "The default is cosine similarity, but if you specify a third argument as `False`, the function uses Euclidean distance. (You may, of course, use and adapt code from class.)\n",
    "\n",
    "*Hint: This week, we've changed our index, and we've changed our distance functions. What part of your search function from last week needs to change, to be compatible with these differences?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f48425c5c34aa1ffcc7b9423b0bb4513",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def search(q,idx,cosine=True):\n",
    "    '''searches for the 10 best matches for a\n",
    "    string query using either cosine similarity\n",
    "    or euclidean distance\n",
    "    \n",
    "    args:\n",
    "        q: query (possibly multi-word) as a string\n",
    "        idx: frequency-based index\n",
    "        cosine: boolean for cosine similarity or\n",
    "            euclidean distance\n",
    "    returns:\n",
    "        list 10 best matching tuples: (score,docID)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    q_tokens = text_prep(q)\n",
    "    q_freq = Counter(q_tokens)\n",
    "    q_vector = {(term, count) for term, count in q_freq.items()}\n",
    "    \n",
    "    scores = []\n",
    "    for i, doc in enumerate(idx):\n",
    "        doc_freqs = doc[2] \n",
    "        \n",
    "        if cosine:\n",
    "            score = cosimfreq(q_vector, doc_freqs)\n",
    "            scores.append((score, i))\n",
    "        else:\n",
    "            score = eucdistfreq(q_vector, doc_freqs)\n",
    "            scores.append((score, i))\n",
    "    \n",
    "    scores.sort(reverse=cosine)\n",
    "    \n",
    "    return scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d57872d8f73db832b1b237295ca8ec88",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r1 = search('the fire wall',docs)\n",
    "\n",
    "# test 3a, 1pt\n",
    "assert type(r1) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46f3b017ddf6e3d4176ad840323bfaa5",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1pt\n",
    "assert len(r1) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31656aa4839bbc4818d358e4a8c8912a",
     "grade": true,
     "grade_id": "q3t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r2 = search('the fire wall',docs,False)\n",
    "\n",
    "# test 3c, 1pt\n",
    "assert r1[0][1] == 23855 and r2[0][1] == 22610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd4d9fb6365949ba36410c71eeda252a",
     "grade": false,
     "grade_id": "cell-2e7312539bcc7858",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosimfreq top 10 results:\teucdistfreq top 10 results:\n",
      "(0.5773502691896258, 23855)\t(1.7320508075688772, 22610)\n",
      "(0.5560384374855327, 101886)\t(1.7320508075688772, 23855)\n",
      "(0.545544725589981, 20432)\t(2.0, 2014)\n",
      "(0.5345224838248488, 217696)\t(2.0, 3034)\n",
      "(0.5333333333333333, 132093)\t(2.0, 3950)\n",
      "(0.5270462766947299, 113628)\t(2.0, 4009)\n",
      "(0.5222329678670935, 182303)\t(2.0, 4280)\n",
      "(0.5222329678670935, 103169)\t(2.0, 4350)\n",
      "(0.5222329678670935, 74900)\t(2.0, 4524)\n",
      "(0.5163977794943222, 247960)\t(2.0, 4830)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's display these\n",
    "print('cosimfreq top 10 results:\\teucdistfreq top 10 results:')\n",
    "[print('{}\\t{}'.format(r1,r2)) for r1, r2 in zip(r1, r2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b424aa40929fa85cac170fe1a689f491",
     "grade": true,
     "grade_id": "q3t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3d, 1pt\n",
    "# I get 0.5773502691896258\n",
    "assert isclose(r1[0][0],.5774,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39a83c4f4491d243b080d590ade3b5d3",
     "grade": true,
     "grade_id": "q3t5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3e, 1pt\n",
    "# I get 1.7320508075688772\n",
    "assert isclose(r2[0][0],1.7321,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1634608a7a41b50b142cd8a03ed624b3",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** This system should have problems related to function words (since we are not removing them). Explain what the problem is. (2 pts)\n",
    "\n",
    "*For full points, be sure your answer makes it clear you understand how the system we've been building **in this assignment** may return results that do not actually meet a user's information need well, as a consequence of function words in the collection and in the query.*\n",
    "\n",
    "*Hint: This problem was discussed in the course videos, so if you're not sure how to approach this, you might benefit from reviewing the videos. For what you write here, you should be able to express this problem in your own words.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a029c7de09ed9485528a3d2cf6a9644",
     "grade": true,
     "grade_id": "q4a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Function words such as \"the,\" \"is,\" \"and,\" etc., are high frequency words, meaning they are commonly used among all the documents. Not removing these function words from the documents can lead to the function to treat function words with the same importance as content words, despite the fact they may not be relevant to the user's query. This confusion would reduce the precision of the model because it will see high frequency words as more important, which may not always be what the query may be requeesting. The best way to do this is to remove the function words form the documents. The removal of these words would lead the funciton to focus on words with more semantic weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e98c9c6705c1c4ea6decbc21f6aa84e7",
     "grade": false,
     "grade_id": "q5q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.** *Demonstrate* the problem that you described in **Question 4** with actual searches on the index and search function you created in questions 1 and 3. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392a5985755976aaa11468a304427cd7",
     "grade": true,
     "grade_id": "q5a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tokens (with function words removed): ['snack', 'bar', 'across', 'mall', 'trump', 'ocean', 'site', 'badly', 'flooded', 'damaged', 'even', 'though', 'basement']\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "def makeFreqIdx2(filename):\n",
    "    doc_index = []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                source_code = int(parts[0])\n",
    "                text = parts[1]\n",
    "                tokens = text_prep2(text)  \n",
    "                freq_counts = Counter(tokens)\n",
    "                freq_set = {(word, count) for word, count in freq_counts.items()}\n",
    "                doc_index.append((source_code, tokens, freq_set))\n",
    "    \n",
    "    return doc_index\n",
    "\n",
    "def text_prep2(input_text):\n",
    "    function_words = {\"the\", \"on\", \"at\", \"and\", \"in\", \"to\", \"of\", \"a\", \"for\", \"with\", \n",
    "                      \"is\", \"that\", \"by\", \"from\", \"it\", \"has\", \"no\", \"was\"}\n",
    "    \n",
    "    normalized = re.sub(r'[^a-zA-Z0-9]', ' ', input_text)\n",
    "    tokens = normalized.lower().split()\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word not in function_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def search2(q, idx, cosine=True):\n",
    "    q_tokens = text_prep2(q)  \n",
    "    q_freq = Counter(q_tokens)\n",
    "    q_vector = {(term, count) for term, count in q_freq.items()}\n",
    "    \n",
    "    scores = []\n",
    "    for i, doc in enumerate(idx):\n",
    "        doc_freqs = doc[2]  \n",
    "        \n",
    "        if cosine:\n",
    "            score = cosimfreq(q_vector, doc_freqs)\n",
    "            scores.append((score, i))\n",
    "        else:\n",
    "            score = eucdistfreq(q_vector, doc_freqs)\n",
    "            scores.append((score, i))\n",
    "    \n",
    "    scores.sort(reverse=cosine)\n",
    "    \n",
    "    return scores[:10]\n",
    "\n",
    "doc_text = 'the snack bar across the mall from the trump on the ocean site was badly flooded and damaged even though it has no basement'\n",
    "processed_tokens = text_prep2(doc_text)\n",
    "print(\"Processed tokens (with function words removed):\", processed_tokens)\n",
    "\n",
    "assert processed_tokens == ['snack', 'bar', 'across', 'mall', 'trump', 'ocean', 'site', \n",
    "                            'badly', 'flooded', 'damaged', 'even', 'though', 'basement'], \\\n",
    "       f\"Tokens after processing: {processed_tokens}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
