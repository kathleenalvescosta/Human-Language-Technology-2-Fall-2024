{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aab6315785a55821fd462327dd36765",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "Linguistics 531<br>\n",
    "Fall 2024<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted.\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. Don't use editors *other* than Jupyter Notebook to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates, or the work of someone on Stack Overflow, or the work of a generative AI model. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: *\\<FILL IN HERE\\>*\n",
    "\n",
    "people I talked to about the assignment: *\\<FILL IN HERE\\>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4473c634f59d0a46ada5cfbb4ca3b04",
     "grade": false,
     "grade_id": "hw7header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #7*\n",
    "\n",
    "**This is due Tuesday, December 3, 2024 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)).\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ac1e372e447955b92046a119fe13730",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f42544eac75a5b2661e9a9777dc7569",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "What I need on my machine to properly grade this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6533f7aefbac103db012fa4f2a36aa",
     "grade": false,
     "grade_id": "cell-ea976235a1443dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c5f3d329570f012501243607da41c1e",
     "grade": false,
     "grade_id": "cell-d62bb3f494727bb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*In the editable cell below, enter the path on your own machine,* then uncomment that line so the notebook works on your machine.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "newbfile = 'train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1133ceea0b96644bf4754c84814221c1",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.** Flesh out the function below that reads in the data and extract sentences in class #2 and class #8. (1 point)\n",
    "\n",
    "You'll return a list of sentences in these two classes. Each sentence should be a single string. (You'll write a function to normalize them in question 2.)\n",
    "\n",
    "You should be able to use concepts and code from last week and earlier in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b5b8f0fee94bc53c19d1bada4b50419",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get28(filename):\n",
    "    '''reads in newB data and extracts classes 2 and 8\n",
    "    \n",
    "    args:\n",
    "        filename: location+name of newB file\n",
    "    returns:\n",
    "        s2: a list of sentences of class 2\n",
    "        s8: a list of sentences of class 8\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    s2 = []\n",
    "    s8 = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split line into class label and sentence\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                label, sentence = parts\n",
    "                # Add sentences to appropriate lists based on class\n",
    "                if label == '2':\n",
    "                    s2.append(sentence)\n",
    "                elif label == '8':\n",
    "                    s8.append(sentence)\n",
    "    \n",
    "    return s2, s8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaba32249da6b452763db9c8ec7a9e74",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "s2,s8 = get28(newbfile)\n",
    "\n",
    "# test 1a, 1pt\n",
    "assert len(s2) == len(s8) == 23071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6704a3b91987387a0d5515ea7590f6d8",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.** Write a function to normalize and tokenize your sentences: convert anything besides upper or lower case letters, numbers, and the percent sign to space, and then split the string on white space. (2 points)\n",
    "\n",
    "You should be able to reuse your code from Assignment 6 for this.\n",
    "\n",
    "*This should now be very familiar. You don't need to overthink this. In a real application, we would probably want to do something fancy to handle Roman letters with accents and diacritics, like á and ñ, which **do** occur in the data sets we'll be using, but we're not doing anything fancy here. If it's not an upper case ASCII letter, or a lower case ASCII letter, or a digit, or the percent sign, convert it to space and then split on any white space. A few document IDs to check, to make sure you're properly normalizing, include 47602, 48327, 48677, and 56254. If you're working in Ubuntu, you can open the `train_orig.txt` file in the built-in text editor (gedit) and perform a RegEx-based \"Find and Replace\" search for a pattern like `[^a-zA-Z0-9%\\s]`, in order to find examples of the kinds of characters your function will need to filter out, and the word tokenization that would result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "213a624c65bbc29444289f79c3a65a20",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    converts anything that is not a letter (upper or lower\n",
    "    case ASCII), number, or percent sign to space, and tokenizes\n",
    "    on white space.\n",
    "    \n",
    "    args:\n",
    "        s: a sentence as a string\n",
    "    returns:\n",
    "        a list of normalized word tokens as strings\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    s = s.lower()\n",
    "    # Replace non-matching chars with space\n",
    "    normalized = re.sub(r'[^a-zA-Z0-9%]', ' ', s)\n",
    "    # Split and filter empty strings\n",
    "    tokens = [token for token in normalized.split() if token]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d55c198faa715277a93ffe719ba3b87c",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "s2sentences = [tokenize(s) for s in s2]\n",
    "s8sentences = [tokenize(s) for s in s8]\n",
    "\n",
    "# test 2a, 1pt\n",
    "assert s8sentences[1214] == ['spurned', 'of', 'a', 'washington', 'dc', 'parade', 'president', 'trump',\n",
    " 'said', 'he', 'now', 'plans', 'to', 'visit', 'paris', 'to', 'see', 'the', 'parade', 'down',\n",
    " 'the', 'avenue', 'des', 'champs', 'lys', 'es', 'to', 'mark', 'the', 'centennial', 'of', 'the',\n",
    " 'end', 'of', 'world', 'war', 'i', 'in', 'mid', 'november']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c33e610d9a54ecdd5b31209bb983dfe5",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1pt\n",
    "assert len(s2sentences[2535]) == 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "278719caf733707a5fc60ff24ddcac20",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** Split the normalized and tokenized sentences in these two categories (using `s2sentences` and `s8sentences` that were output from question 2) into training and test sets where the *last* 1000 sentences comprise each test set and the rest make up the training sets. (3 points)\n",
    "\n",
    "Pay attention to the variable names that are in the comments.\n",
    "\n",
    "Again, this should seem very similiar to something you did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1928e3f8a18855addfd7989ac2002082",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#train2 = all but the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "train2 = s2sentences[:-1000]\n",
    "\n",
    "#test2 = the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "test2 = s2sentences[-1000:]\n",
    "\n",
    "#train8 = all but the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "train8 = s8sentences[:-1000]\n",
    "\n",
    "#test8 = the last 1000 sentences\n",
    "# YOUR CODE HERE\n",
    "test8 = s8sentences[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0211e46708e813c4a732e4ec92783907",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3a, 1pt\n",
    "assert len(train2) == len(train8) == 22071 and len(test2) == len(test8) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bc58a30a90f47cf703576525a48e383",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1pt\n",
    "assert train8[15000][:5] == ['illustration', 'caption', 'sarah',\n",
    "                             'ricegetty', 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b34ffd7a56f482130f2a6a3cfba6cad8",
     "grade": true,
     "grade_id": "q3t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3c, 1pt\n",
    "assert test8[55][:5] == ['a', 'plan', 'being', 'considered', 'by']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "933e0cdfef8597c8020523f751304f47",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** Extract the set of all normalized and tokenized words in class 2, class 8, and in both classes together. (3 points)\n",
    "\n",
    "You should include both the test and training sets for each class; that is, you can use `s2sentences` and `s8sentences` from question 2 as your input, not the separate test and training sets that were created as output in **Question 3**. We'll use the testing and training partitions in later questions.\n",
    "\n",
    "*Hint: [Set comprehensions](https://www.pythonforbeginners.com/basics/set-comprehension-in-python) may be helpful here. Also, once you have `vocab2` and `vocab8`, what's an easy way to create `vocab28`?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "018111ca488e58ca7fb40a0726a94424",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#vocab2 = set of words in class 2\n",
    "# YOUR CODE HERE\n",
    "vocab2 = set(\n",
    "    token.lower()  # Convert to lowercase before adding to set\n",
    "    for sentence in s2sentences\n",
    "    if isinstance(sentence, str)\n",
    "    for token in re.sub(r'[^a-zA-Z0-9%]', ' ', sentence).split()\n",
    "    if token  # Filter out empty tokens\n",
    ")\n",
    "\n",
    "#vocab8 = set of all words in class 8\n",
    "# YOUR CODE HERE\n",
    "vocab8 = set(\n",
    "    token.lower()  # Convert to lowercase before adding to set\n",
    "    for sentence in s8sentences\n",
    "    if isinstance(sentence, str)\n",
    "    for token in re.sub(r'[^a-zA-Z0-9%]', ' ', sentence).split()\n",
    "    if token  # Filter out empty tokens\n",
    ")\n",
    "\n",
    "#vocab28 = set of all words in class 2 and 8\n",
    "# YOUR CODE HERE\n",
    "vocab28 = vocab2.union(vocab8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f12ee00da44ecc349845f255d56f22e4",
     "grade": true,
     "grade_id": "q4t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This will obviously depend on your normalization function; if you're\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#  having trouble getting these numbers, check that your normalization\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  function is really converting all the appropriate characters to space.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# test 4a, 1pt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vocab2) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m20701\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This will obviously depend on your normalization function; if you're\n",
    "#  having trouble getting these numbers, check that your normalization\n",
    "#  function is really converting all the appropriate characters to space.\n",
    "\n",
    "# test 4a, 1pt\n",
    "assert len(vocab2) == 20701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "769924a313f7030df3dd48710112e8e7",
     "grade": true,
     "grade_id": "q4t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4b, 1pt\n",
    "assert len(vocab8) == 21353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eaa4182a3d59b2a9c1dc22735be4eae7",
     "grade": true,
     "grade_id": "q4t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 4c, 1pt\n",
    "assert len(vocab28) == 27944"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c98c0a75c7d3e3c5e5c838762e3dfc97",
     "grade": false,
     "grade_id": "q5q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.** Write a function that will create a multinomial Naive Bayes model from training data. (3 points)\n",
    "\n",
    "The function will take two arguments: a list of normalized and tokenized sentences in some class (that is, `train2` or `train8` from question 3) plus a list of all possible words (in any class&mdash;`vocab28` from question 4). Look at the `assert` statements below to see how the function will behave.\n",
    "\n",
    "The function will return a dictionary from each possible word to its log probability value in that training data. Use add-one smoothing to make sure all possible words have a non-zero (non-infinite in log space) value.\n",
    "\n",
    "*Hint: Are you counting words? A `Counter()` may be a good option for this task. Note that in order to implement smoothing  with a `Counter()`, you may need to tackle this in two steps. Think about the two data structures that are inputs to this function and consider what each of them might help you with for this task. The Review & Mastery activity for this week may give you some hints.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dfd096da77cd28e0aca737ddbb01f63",
     "grade": false,
     "grade_id": "q5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeModel(sentences,vocabulary):\n",
    "    '''\n",
    "    returns a multinomial naive bayes model for a list\n",
    "    of sentences and a set of possible words. Values are\n",
    "    expressed as log probabilities.\n",
    "    \n",
    "    Uses add-one smoothing to make sure all possible words\n",
    "    have a non-zero (non-infinite in log space) value.\n",
    "    \n",
    "    args:\n",
    "        sentences: a list of training sentences\n",
    "        vocabulary: a set of all possible words\n",
    "    returns:\n",
    "        a dictionary from possible words to that word's log probability\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    word_counts = Counter({word: 1 for word in vocabulary})\n",
    "    \n",
    "    # Count words in all sentences using Counter\n",
    "    for sentence in sentences:\n",
    "        word_counts.update(sentence)\n",
    "    \n",
    "    # Calculate total words including smoothing\n",
    "    total_words = sum(word_counts.values())\n",
    "    \n",
    "    # Convert to log probabilities using numpy\n",
    "    model = {word: np.log(count/total_words) \n",
    "            for word, count in word_counts.items()}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6025d3b27a44522d8400d9a0d5c69bb",
     "grade": true,
     "grade_id": "q5t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model2 = makeModel(train2,vocab28)\n",
    "model8 = makeModel(train8,vocab28)\n",
    "\n",
    "# test 5a, 1pt\n",
    "assert len(model2) == len(model8) == 27944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81b5960f337e35771e218b2e94606057",
     "grade": true,
     "grade_id": "q5t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5b, 1pt\n",
    "# I get -11.313875043456596\n",
    "assert isclose(model2['hats'],-11.3139,abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f7e10de3e28858808607d648003903b",
     "grade": true,
     "grade_id": "q5t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5c, 1pt\n",
    "# I get -11.455193945760294\n",
    "assert isclose(model8['hats'], -11.4552, abs_tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c9c4c9802f18e7dc3a9cb914071d611",
     "grade": false,
     "grade_id": "q6q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "**6.** Fill out the function below to apply a pair of multinomial naive bayes models (that your function in question 5 created) to a list of normalized and tokenized sentences for testing (that is, we'll use `test2` and `test8` for this). (2 points)\n",
    "\n",
    "Just like our evaluation functions in the last homework, this evaluation function reports the number of sentences that match the second model better.\n",
    "\n",
    "You will *not* need the Bayes correction because there are an equal number of items in both classes in the test data.\n",
    "\n",
    "*Hint: If you have a Naive Bayes model, how do you evaluate the predicted probability of a test document according to that model? How much code can you re-use from this week's in-class notebook?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09c062cc7a1866fd7d7f48dfb2af78f4",
     "grade": false,
     "grade_id": "q6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def applyModels(modelA,modelB,sentences):\n",
    "    '''\n",
    "    Determine which of two models fits the test data better.\n",
    "    \n",
    "    args:\n",
    "        modelA, modelB: Naive Bayes models as created by makeModel()\n",
    "        sentences: a list of normalized & tokenized sentences\n",
    "            as created by tokenize()\n",
    "    returns:\n",
    "        the number of sentences that fit modelB better\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dead3b59504b73d05a577333bea0e271",
     "grade": true,
     "grade_id": "q6t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note that because we've written this function to retun the number\n",
    "#  of sentences that fit *model B* better, we need to be sure that\n",
    "#  the category for model B always matches the category of the test set.\n",
    "\n",
    "res2 = applyModels(model8,model2,test2)\n",
    "res8 = applyModels(model2,model8,test8)\n",
    "\n",
    "# test 6a, 1pt\n",
    "assert res2 == 729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "980d78bf2083f789aa0735160657cc70",
     "grade": true,
     "grade_id": "q6t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 6b, 1pt\n",
    "assert res8 == 645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a95af50755d0532e57bdf50358fe7b1",
     "grade": false,
     "grade_id": "q7q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**7.** Summarize the results of your multinomial Naive Bayes models and discuss. (2 points)\n",
    "\n",
    "How well did the model do here? How did this model do, compared to the previous two models that we tried (namely, classifying solely by document length, and then classifying based on a normal statistical distribution)? Why do you think you're seeing this kind of performance, with this kind of model?\n",
    "\n",
    "*Hint: If you're not sure how to approach this, re-watch the beginning of video 7.2 and the end of 7.4, paying attention to the assumptions that the multinomial Naive Bayes model makes about the documents within each class. What assumptions did our **previous** models make about the documents within each class? Does the Naive Bayes classification model make any of the same assumptions? How well do those assumptions hold of this data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "beea62aca81f61a18e6f29ddf9ff49c5",
     "grade": true,
     "grade_id": "q7a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
