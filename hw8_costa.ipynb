{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aab6315785a55821fd462327dd36765",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='https://hammondm.github.io/hltlogo1.png' style=\"float:right\">\n",
    "Linguistics 531<br>\n",
    "Fall 2024<br>\n",
    "Jackson\n",
    "\n",
    "## Things to remember about any homework assignment:\n",
    "\n",
    "1. For this assignment, you will edit this jupyter notebook and turn it in. Do not turn in pdf files or separate `.py` files.\n",
    "1. Late work is not accepted.\n",
    "1. Given the way I grade, you should try to answer *every* question, even if you don't like your answer or have to guess.\n",
    "1. You may *not* use `python` modules that we have not already used in class. (For grading, it needs to be able to run on my machine, and the way to do that is to limit yourself to the modules we've discussed and that are loaded into the Notebook.)\n",
    "1. Don't use editors *other* than Jupyter Notebook to work on and submit your assignment, since they will mangle the autograding features: Google Colab, or even just editing the `.ipynb` file as a plain text file. Diagnosing and fixing that kind of problem takes a lot of my time, and that means less of my time to offer constructive feedback to you and to other students.\n",
    "1. You may certainly talk to your classmates about the assignment, but everybody must turn in *their own* work. It is not acceptable to turn in work that is essentially the same as the work of classmates, or the work of someone on Stack Overflow, or the work of a generative AI model. Using someone else's code and simply changing variable or object names is *not* doing your own work.\n",
    "1. All code must run. It doesn't have to be perfect, it may not do all that you want it to do, but it must run without error. Code that runs with errors will get no credit from the autograder.\n",
    "1. Code must run in reasonable time. Assume that if it takes more than *5 minutes* to run (on your machine), that's too long.\n",
    "1. Make sure to select `restart, run all cells` from the `kernel` menu when you're done and before you turn this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my name: Kathleen Costa\n",
    "\n",
    "people I talked to about the assignment: N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfcfa1e57b354b7de6001165a2383915",
     "grade": false,
     "grade_id": "h8header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework #8\n",
    "\n",
    "**This is due Tuesday, December 10, 2024 at noon (Arizona time).**\n",
    "\n",
    "This assignment continues with the `NewB` corpus (downloadable [here](https://github.com/JerryWei03/NewB)).\n",
    "\n",
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ef8226ca7ba813bedc100456453bc3a",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "import pandas as pd\n",
    "\n",
    "# Even though your document vectors will be sorted lists,\n",
    "#  using a Counter() will be an efficient way to get there.\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f42544eac75a5b2661e9a9777dc7569",
     "grade": false,
     "grade_id": "cell-85e7bb5f0c446cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**As before, this section is for autograding:**\n",
    "\n",
    "What I need on my machine to properly grade this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6533f7aefbac103db012fa4f2a36aa",
     "grade": false,
     "grade_id": "cell-ea976235a1443dc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Path on my own machine, needed for GRADING\n",
    "newbfile = '/home/ejackson1/Downloads/linguistics/NewB/train_orig.txt'\n",
    "\n",
    "# ie, DON'T CHANGE THIS CELL, CHANGE THE ONE BELOW!\n",
    "#  If you change *this* cell, the autograding is likely to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c5f3d329570f012501243607da41c1e",
     "grade": false,
     "grade_id": "cell-d62bb3f494727bb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*In the editable cell below, enter the path on your own machine,* then uncomment that line so the notebook works on your machine.\n",
    "\n",
    "**BEFORE YOU SUBMIT to D2L, remember to comment out *your* path again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR path\n",
    "newbfile = 'train_orig.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9160f154460f807c3520411d31c1f523",
     "grade": false,
     "grade_id": "q1q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.** Fill in the `get49()` function below to read in the data and extract sentences in class #4 and class #9. (3 points)\n",
    "\n",
    "This should be very similar to what you've done on past assignments, just with a new pair of classes. This function will also include some of the kinds of operations that we used to create our document vectors (which are a form of document-term index).\n",
    "\n",
    "You should use the `tokenize()` function **within `get49()`** to process the text as we have on previous homeworks: do not stem or remove stop words; keep only numbers, upper and lower case ASCII letters, and the percent sign. You can likely copy your function from assignment 7. Although there shouldn't be any upper case letters in this data set, it's always a good idea to check and normalize your data before assuming what's there.\n",
    "\n",
    "Convert the sentences to raw term frequency counts for each sentence. (In past weeks and in this week's class notebook, some of the things we've done use *tf-idf* counts, but for this homework, we're keeping things simple and just using raw frequency counts. If you're curious to see how using *tf-idf* counts would affect this week's data, you're welcome to try that in another notebook. It's not required, though it might provide useful points for your discussion in question 8.)\n",
    "\n",
    "Your function should return two lists, one for each class. See the docstring and cells below for clarity. Each item in these lists corresponds to a document (ie, a sentence in this data). For each document, you should have an **alphabetically sorted** list of two-item tuples where the first item is a normalized word and the second is the frequency count for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5c9940ce5c21f552288279698cb4720",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get49(filename):\n",
    "    '''\n",
    "    calculates word count vectors for all documents in\n",
    "    classes #4 and #9\n",
    "    \n",
    "    args:\n",
    "        filename: location of train_orig.txt\n",
    "        \n",
    "    returns:\n",
    "        two lists, one each for category 4 and 9;\n",
    "            within each category's list, there should be a list (one per\n",
    "            document) of tuples (<normalized word>, <count>), sorted\n",
    "            alphabetically by the normalized word\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    category4_docs = []\n",
    "    category9_docs = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                category, text = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            if category not in ['4', '9']:\n",
    "                continue\n",
    "            \n",
    "            word_counts = Counter(tokenize(text))\n",
    "            sorted_counts = sorted(word_counts.items())\n",
    "            \n",
    "            if category == '4':\n",
    "                category4_docs.append(sorted_counts)\n",
    "            else:\n",
    "                category9_docs.append(sorted_counts)\n",
    "    \n",
    "    return category4_docs, category9_docs\n",
    "\n",
    "def tokenize(s):\n",
    "    '''\n",
    "    converts anything that is not a letter (upper or lower\n",
    "    case ASCII), number, or percent sign to space, and tokenizes\n",
    "    on white space.\n",
    "    \n",
    "    args:\n",
    "        s: a sentence as a string\n",
    "    returns:\n",
    "        a list of normalized word tokens as strings\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9% ]\", \" \", s) \n",
    "    cleaned = re.sub(r\" +\", \" \", cleaned) \n",
    "    return cleaned.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c3c4951350cb260732a7f2d9a61af35",
     "grade": true,
     "grade_id": "q1t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ws4,ws9 = get49(newbfile)\n",
    "\n",
    "# test 1a, 1pt\n",
    "assert len(ws4) == len(ws9) == 23071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab03da68c61566df6d8c1ef2337a7aa5",
     "grade": true,
     "grade_id": "q1t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sample = \"  More than  28%  of sentences in this   data contain�the president's name.\"\n",
    "\n",
    "# test 1b, 1pt\n",
    "# This is just a separate test of your tokenize() function\n",
    "assert tokenize(sample) == ['More',  'than', '28%', 'of', 'sentences', 'in', 'this',\n",
    " 'data', 'contain', 'the', 'president', 's', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62cd70ed985d8c98e47e44fc9ce21ba1",
     "grade": true,
     "grade_id": "q1t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 1c, 1pt\n",
    "# Be sure you've applied your tokenize() function to each document/sentence\n",
    "#  before creating these document vectors.\n",
    "assert ws9[21321] == [('all', 1),('avoid', 1),('crossing', 1),\n",
    " ('donald', 1),('i', 1),('in', 1),('m', 1),('other', 1),\n",
    " ('s', 1),('saying', 1),('t', 1),('the', 1),('to', 1),\n",
    " ('trump', 1),('words', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attacks', 1),\n",
       " ('criticize', 1),\n",
       " ('if', 1),\n",
       " ('president', 1),\n",
       " ('the', 1),\n",
       " ('them', 1),\n",
       " ('they', 1),\n",
       " ('trump', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws4[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fa225ac06be4a3cfaeabb4b9ccffb98",
     "grade": false,
     "grade_id": "q2q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.** Separate the last 300 documents for each class into separate test sets and use the remainder for training. (3 points)\n",
    "\n",
    "This task should be familiar from previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11b2fafc6f188c9b9803f9243447badf",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#train4 = all but last 300 items for training\n",
    "# YOUR CODE HERE\n",
    "cat4_docs, cat9_docs = get49('train_orig.txt')\n",
    "train4 = cat4_docs[:-300]\n",
    "\n",
    "#test4 = last 300 items for test\n",
    "# YOUR CODE HERE\n",
    "test4 = cat4_docs[-300:]\n",
    "\n",
    "#train9 = all but last 300 items for training\n",
    "# YOUR CODE HERE\n",
    "train9 = cat9_docs[:-300]\n",
    "\n",
    "#test9 = last 300 items for test\n",
    "#YOOUR CODE HERE\n",
    "test9 = cat9_docs[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5509c14bbcb2f3755207a1e92a92888",
     "grade": true,
     "grade_id": "q2t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2a, 1 pt\n",
    "assert ws9[-300] == test9[0] and ws4[-300] == test4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "256630e167c6bd589341353a15f7071f",
     "grade": true,
     "grade_id": "q2t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2b, 1 pt\n",
    "assert len(train4) == len(train9) == 22771 and len(test4) == len(test9) == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0547c14bb538ba7831780ed1bea4acf8",
     "grade": true,
     "grade_id": "q2t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 2c, 1 pt\n",
    "assert train4[5342] == [('added', 1), ('after', 1), ('an', 1), ('and', 1), ('as', 1),\n",
    " ('bolton', 1), ('but', 1), ('consolidate', 1), ('cuba', 1), ('lumped', 1), ('maduro', 1),\n",
    " ('missed', 1), ('nation', 1), ('nicaragua', 1), ('nicol', 1), ('of', 1), ('on', 1),\n",
    " ('opportunity', 1), ('piccone', 1), ('president', 1), ('pressure', 1), ('s', 1),\n",
    " ('that', 2), ('the', 1), ('to', 1), ('troika', 1), ('trump', 1), ('tyranny', 1),\n",
    " ('venezuelan', 1), ('with', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c05f8e05cb3efa3842442d67d5e2b514",
     "grade": false,
     "grade_id": "q3q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.** Write a function to calculate the centroid of a list of document vectors that are in the data structure we're using. (4 points)\n",
    "\n",
    "Check the docstring as well as the cells below to see how this should be applied. Remember, we want to calculate the centroid for our class 4 sentences and for our class 9 sentences. As we saw in the lecture, though, our vocabulary counts constitute sparse lexical vectors. \n",
    "\n",
    "*(Hint: You may find that a dictionary or Counter is a helpful data structure for this calculation. The bit of testing that I've done happens to show that dictionaries are much faster than Counters here, but it does depend on the details of how you implement it!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e54baada43af8e5bd30a7e2f3f17812c",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sparseCentroid(vectors):\n",
    "    '''\n",
    "    takes a list of sparse document vectors and calculates\n",
    "    the centroid\n",
    "    \n",
    "    args:\n",
    "        vectors: a list of lists of sorted (word,count) tuples\n",
    "    returns:\n",
    "        centroid as a sorted list of (word,avg) tuples\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    if not vectors:\n",
    "        return []\n",
    "        \n",
    "    combined_counts = Counter()\n",
    "    for vector in vectors:\n",
    "        combined_counts.update(dict(vector))\n",
    "\n",
    "    num_vectors = len(vectors)\n",
    "    centroid = [(word, count / num_vectors) for word, count in combined_counts.items()]\n",
    "\n",
    "    return sorted(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "892551876e6c8e3d1176ca6431d7870a",
     "grade": true,
     "grade_id": "q3t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "centroid4 = sparseCentroid(train4)\n",
    "centroid9 = sparseCentroid(train9)\n",
    "\n",
    "# test 3a, 1 pt\n",
    "# This depends on the terms that are in class 4, right?\n",
    "assert len(centroid4) == 23576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db788a96887f641da46c2734ed482216",
     "grade": true,
     "grade_id": "q3t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3b, 1 pt\n",
    "# Likewise, this depends on the terms in class 9.\n",
    "assert len(centroid9) == 18088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29e4eb898d8b0372a4c75363f2a9783d",
     "grade": true,
     "grade_id": "q3t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "c4dict = dict(centroid4)\n",
    "c9dict = dict(centroid9)\n",
    "\n",
    "# test 3c, 1 pt\n",
    "assert isclose(c4dict['hat'],0.0006587,abs_tol=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a751cdb20a6b08233c5594fa6ed28445",
     "grade": false,
     "grade_id": "cell-e751d62f812e4847",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3059154187343551"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check:\n",
    "# I get 1.3059154187343551\n",
    "# Remember, these are raw weights, not tf-idf scores, so this is the average number of\n",
    "#  times the word 'the' occurs in documents in class 4. If your numbers (including this\n",
    "#  one) are off, be sure you're just using raw weights. We would otherwise expect a\n",
    "#  common word like 'the' to have a very low tf-idf score, right?\n",
    "c4dict['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "641e444be6c39488715d53020e6cca99",
     "grade": true,
     "grade_id": "q3t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 3d, 1 pt\n",
    "assert isclose(c9dict['hat'],0.0003513,abs_tol=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "725735d2e1d890a2b888fab1bcc06741",
     "grade": false,
     "grade_id": "fromclass1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here's an updated version of cosine similarity we developed for these in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c89897b01590657cc458900d7685f42c",
     "grade": false,
     "grade_id": "fromclass2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cosimfreq(d1,d2):\n",
    "    '''\n",
    "    Calculate cosine similarity for two sparse document vectors,\n",
    "    using dicts for calculations and Numpy's built-in `.norm`.\n",
    "    \n",
    "    args:\n",
    "        d1, d2: lists of (word, freq) tuples\n",
    "    returns:\n",
    "        a float from 0 to 1 expressing the similarity of these vectors\n",
    "        (values closer to 1 are more similar)\n",
    "    '''\n",
    "    # concise, AND efficient-ish!\n",
    "    d1,d2 = dict(d1), dict(d2)\n",
    "    dot_product = sum(d1[word] * d2[word] for word in d1.keys() & d2.keys())\n",
    "    magnitudes = np.linalg.norm([*d1.values()]) * np.linalg.norm([*d2.values()])\n",
    "    if magnitudes == 0: return 0\n",
    "    return dot_product/magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fb03ae51411554a85f1c4b05d26b985",
     "grade": false,
     "grade_id": "q4q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.** Now we do Rocchio classification for the two test sets. Write a function as described in the docstring below, using centroids as output by `sparseCentroid()` (question 3&mdash;see the `assert` statements below) and using `cosimfreq()` from above. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0187e357d48ce11b9551ee6e9e54f48",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def testSet(centroid1,centroid2,vectors):\n",
    "    '''\n",
    "    takes two centroids and a list of document vectors\n",
    "    and returns how many vectors are closer to the second\n",
    "    centroid (using cosimfreq).\n",
    "    \n",
    "    Everything is represented as lists of (word,freq) tuples.\n",
    "    \n",
    "    args:\n",
    "        centroid1: centroid of class 1\n",
    "        centroid2: centroid of class 2\n",
    "        vectors: list of documents\n",
    "    returns:\n",
    "        integer number of how many of the vectors are\n",
    "        closer to the second centroid\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    closer_to_second = 0\n",
    "    v1 = 0\n",
    "    v2 = 0\n",
    "\n",
    "    for vector in vectors:\n",
    "        sim1 = cosimfreq(centroid1, vector)\n",
    "        sim2 = cosimfreq(centroid2, vector)\n",
    "        \n",
    "        if sim2 > sim1:\n",
    "            closer_to_second += 1\n",
    "\n",
    "    return closer_to_second\n",
    "\n",
    "#This code lead the kernel to die. I've tried several different ways, including from the nb10vectorspace assignment. I've tried to alter\n",
    "#the other code to see if that might be the issue, but I think the kernel simply can't handle large amounts of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d06ac250404084170e5cb7f758ba944",
     "grade": true,
     "grade_id": "q4t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#this may take 2-3 minutes\n",
    "result4 = testSet(centroid9,centroid4,test4)\n",
    "\n",
    "# test 4a, 1 pt\n",
    "# Out of the 300 item test set, I get that 202 are evaluated correctly.\n",
    "assert isclose(result4,202,abs_tol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c941b5163f377fb88a91cb4f9861f4bf",
     "grade": true,
     "grade_id": "q4t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#this may take 2-3 minutes\n",
    "result9 = testSet(centroid4,centroid9,test9)\n",
    "\n",
    "# test 4a, 1 pt\n",
    "# Out of the 300 item test set, I get that 183 are evaluated correctly.\n",
    "assert isclose(result9,183,abs_tol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b09c91bf12f3a98060d528bd8a9b65bf",
     "grade": false,
     "grade_id": "rocchioSummary1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's summarize the performance of the Rocchio method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f06e4cd004764b8f8396a33255ce86f2",
     "grade": false,
     "grade_id": "rocchioSummary2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testSize = 300\n",
    "print(\"Rocchio method summary:\")\n",
    "print(\"Test set 4, {} out of {} correct, {:.0%}\".format(result4, testSize, result4/testSize))\n",
    "print(\"Test set 9, {} out of {} correct, {:.0%}\".format(result9, testSize, result9/testSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98cfae9856986aed92095f4676bd6e4c",
     "grade": false,
     "grade_id": "q5q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.** Now we'll work up to a full kNN classification. This involves a *lot* of calculations, so let's reduce the data a bit. For each class, set aside 1000 training items, 100 development items, and 100 test items, as described below. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9680bfa82d8b57918f9c2dd82ccf8d0b",
     "grade": false,
     "grade_id": "q5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#newtrain4 = first 1000\n",
    "# YOUR CODE HERE\n",
    "newtrain4 = cat4_docs[:1000] \n",
    "\n",
    "#newdev4 = next 100\n",
    "# YOUR CODE HERE\n",
    "newdev4 = cat4_docs[1000:1100] \n",
    "\n",
    "#newtest4 = next 100\n",
    "# YOUR CODE HERE\n",
    "newtest4 = cat4_docs[1100:1200]\n",
    "\n",
    "#newtrain9 = first 1000\n",
    "# YOUR CODE HERE\n",
    "newtrain9 = cat9_docs[:1000]\n",
    "\n",
    "#newdev9 = next 100\n",
    "# YOUR CODE HERE\n",
    "newdev9 = cat9_docs[1000:1100]\n",
    "\n",
    "#newtest9 = next 100\n",
    "# YOUR CODE HERE\n",
    "newtest9 = cat9_docs[1100:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a7c2b037ae840c1667c5703baed0d96",
     "grade": true,
     "grade_id": "q5t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5a, 1 pt\n",
    "assert (len(newtrain4) == len(newtrain9) == 1000) and \\\n",
    "    (len(newtest4) == len(newtest9) == len(newdev4) == len(newdev9) == 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b8c06f14919230514b7c128c703a451",
     "grade": true,
     "grade_id": "q5t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5b, 1 pt\n",
    "assert newdev4[0] == ws4[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9440817ea36109fcebae5b039c0c304d",
     "grade": true,
     "grade_id": "q5t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 5c, 1 pt\n",
    "assert newtest9[0] == ws9[1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1479c21a9750ce94554d6f3ddd2db800",
     "grade": false,
     "grade_id": "q6q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**6.** Now we write a function that takes: two lists of training documents, represented as above; a class label for each of these lists of training documents; and a single document for development testing, that we will compare to these lists of documents. This function returns a single ordered list of cosine similarity scores reflecting the test documents that are closest to the single development document. (4 points)\n",
    "\n",
    "To be clear, the two lists of documents will be our full *training* sets (each with 1000 documents), and the single document will be from our *development* set. We'll call this function from the code that we write in question 7, which will loop through the documents in our development set, in order to find the best value for $k$ for each development document. It may be helpful to look at question 7 before you complete the code here, to see how this function will be used there.\n",
    "\n",
    "In the original lists, each item is a document vector (an ordered list of tuples). In the returned list, each item is a tuple of cosine similarity score (of one training document and the development document) and the class label of that training document. (We'll keep track of the class label of the *development document* in the function we write in Question 7.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78b28dbd0e7d3a34ae9fa62e67bc97f5",
     "grade": false,
     "grade_id": "q6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeList(trainlist1,label1,trainlist2,label2,devdoc):\n",
    "    '''\n",
    "    ranks the elements of two lists in a single\n",
    "    new list in terms of cosine similarity score\n",
    "    with respect to a single document.\n",
    "    \n",
    "    args:\n",
    "        trainlist1:  document vectors of one class for training\n",
    "        label1:      the label of that class\n",
    "        trainlist2:  document vectors of a second class for training\n",
    "        label2:      label of the second class\n",
    "        devdoc:      a single document vector from a development list\n",
    "    returns:\n",
    "        a ranked list of tuples, ranked from most\n",
    "        similar to least similar: (score,class label)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    scores = []\n",
    "    for doc in trainlist1:\n",
    "        sim = cosimfreq(doc, devdoc)\n",
    "        scores.append((sim, label1))\n",
    "    for doc in trainlist2:\n",
    "        sim = cosimfreq(doc, devdoc)\n",
    "        scores.append((sim, label2))\n",
    "    scores.sort(reverse=True)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c23fc786d3cc8d2a1599988fa3cddfa6",
     "grade": true,
     "grade_id": "q6t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res = makeList(newtrain4,4,newtrain9,9,newdev4[0])\n",
    "\n",
    "# test 6a, 1 pt\n",
    "# 1000 training documents from each class means that we should have a list\n",
    "#  of 2000 similarities returned.\n",
    "assert len(res) == 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74bc06c009d087facb5cb71f814f510c",
     "grade": true,
     "grade_id": "q6t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 6b, 1 pt\n",
    "assert type(res[0]) == tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a47c33cbbc403a54ca495cca8ed81d0e",
     "grade": true,
     "grade_id": "q6t3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 6c, 1 pt\n",
    "assert all([(label == 4 or label == 9) for similarity, label in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64da95de0e325c3c0367ccf6722b5ee5",
     "grade": true,
     "grade_id": "q6t4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test 6d, 1 pt\n",
    "assert res[505][0] > res[506][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a5b184c7ee72d851d8ae9722e61a274",
     "grade": false,
     "grade_id": "q7q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**7.** Now we use the training sets and development sets from question 5 with the function you created in question 6 to determine the best value for $k$ for all the items in the development sets, and then choose the best $k$ overall. (2 points)\n",
    "\n",
    "The following function takes two training sets and a single development set as arguments and returns a dictionary of $k$ values containing a count for how many times the *second* category is chosen by the kNN method, where $k$ varies over odd numbers from $1$ to $50$.\n",
    "\n",
    "*(Hint: It may be helpful to express the possible values of $k$ as a `range()`.)*\n",
    "\n",
    "This function should step through the documents in the development set, calling the `makeList()` function that you wrote in question 6 for each one.\n",
    "\n",
    "*(Another hint: You'll be stepping through the possible values of $k$, and stepping through the documents in the development set. What is the most efficient way to combine these loops? Think about how you can avoid re-calculating the same things.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03f25ca5ae252e61df22210e27b8f052",
     "grade": false,
     "grade_id": "q7a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def findK(trainlist1,label1,trainlist2,label2,devset):\n",
    "    '''\n",
    "    returns kNN counts for different choices of k\n",
    "    \n",
    "    args:\n",
    "        trainlist1:  one list of documents for training\n",
    "        label1:      label of the first training list\n",
    "        trainlist2:  another list of documents for training\n",
    "        label2:      label of the other training list\n",
    "        devset:      one list of development documents\n",
    "    returns:\n",
    "        a dictionary from values of k to counts\n",
    "        for how many times the second class was\n",
    "        chosen.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    k_counts = {}\n",
    "    \n",
    "    k_values = range(1, 50, 2)\n",
    "    \n",
    "    for dev_doc in devset:\n",
    "        ranked_list = makeList(trainlist1, label1, trainlist2, label2, dev_doc)\n",
    "        \n",
    "        for k in k_values:\n",
    "            top_k = ranked_list[:k]\n",
    "            label2_count = sum(1 for _, label in top_k if label == label2)\n",
    "            k_counts[k] = k_counts.get(k, 0) + (label2_count > k/2)\n",
    "    \n",
    "    return k_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd2adfcae079e8ed7985b0d0c6686ab3",
     "grade": true,
     "grade_id": "q7t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res944 = findK(newtrain9,9,newtrain4,4,newdev4)\n",
    "res499 = findK(newtrain4,4,newtrain9,9,newdev9)\n",
    "\n",
    "# test 7a, 1 pt\n",
    "# Odd numbers from 1 to 50--that's 25 possible values for k\n",
    "assert len(res944) == len(res499)== 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a3b31f131c5d49a2609ba66277789e5",
     "grade": false,
     "grade_id": "ktable1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can add those values together to see which value of $k$ scores the highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ff5768fc30adb40be866bcf54dfbfc0",
     "grade": false,
     "grade_id": "ktable2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame([res944,res499]).transpose()\n",
    "results['sum'] = results.sum(axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a890343920600d61c0745778c8c0473e",
     "grade": true,
     "grade_id": "q7t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's choose the best k: the label of the row with the highest times it made\n",
    "#  the right prediction for the development set.\n",
    "bestk = results['sum'].idxmax()\n",
    "\n",
    "# test 7b, 1 pt\n",
    "assert bestk == 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "986e13e9cb2f8c5e61a375a687528870",
     "grade": false,
     "grade_id": "finalk1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now run kNN on the test sets with the best value of $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05825bdaf668b89651c4e8fabdc81d50",
     "grade": false,
     "grade_id": "finalk2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "newTestSize=100\n",
    "\n",
    "testres944 = findK(newtrain9,9,newtrain4,4,newtest4)\n",
    "testres499 = findK(newtrain4,4,newtrain9,9,newtest9)\n",
    "good4 = testres944[bestk]\n",
    "good9 = testres499[bestk]\n",
    "\n",
    "print(\"kNN method summary:\")\n",
    "print(\"Test set 4, {} out of {} correct, {:.0%}\".format(good4, newTestSize, good4/newTestSize))\n",
    "print(\"Test set 9, {} out of {} correct, {:.0%}\".format(good9, newTestSize, good9/newTestSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2af8d1ed4025b35d6417d666db917f49",
     "grade": false,
     "grade_id": "q8q",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**8.** Summarize the results of your Rocchio and kNN classification systems and discuss. Can you say anything about why you get the results you do? The Review & Mastery exercises for this week may suggest some relevant things to consider. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8478d5f8ad28fba977b42b17866f8c6d",
     "grade": true,
     "grade_id": "q8a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Even though my Rocchio classification system didn't work as expected, I can see that 202 and 183 of the 300 test items performed well with this classification system. Centroids are used to represent each of the classes and define the boundaries of each of these classes. In kNN classification systems, classifications are made using local document neighborhoods. In other words, the proximity of the documents is used to make predictions about the documents. kNN also looks at the individual similarities of a document rather than looking at the overall class pattern. In this assignment, the best value of k = 19. Since there is such a high number associated with the Rocchio classification (202 and 183 out of 300), it is implied that the classes have distinct centroid patterns. These results make sense due to the classification nature of each of these classification systems, since kNN looks at the local similarities while Rocchio looks at the class characteristics as a whole through the scope of centroids. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
